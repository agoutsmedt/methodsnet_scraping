@article{adi2014,
  title = {Elite {{Tweets}}: {{Analyzing}} the {{Twitter Communication Patterns}} of {{Labour Party Peers}} in the {{House}} of {{Lords}}: {{Elite Tweets}}},
  shorttitle = {Elite {{Tweets}}},
  author = {Adi, Ana and Erickson, Kristofer and Lilleker, Darren G.},
  year = {2014},
  month = mar,
  journal = {Policy \& Internet},
  volume = {6},
  number = {1},
  pages = {1--27},
  issn = {19442866},
  doi = {10.1002/1944-2866.POI350},
  urldate = {2022-05-23},
  abstract = {The microblogging platform Twitter has gained notoriety for its status as both a communication channel between private individuals and as a public forum monitored by journalists, the public, and the state. Its potential application for political communication has not gone unnoticed; politicians have used Twitter to attract voters, interact with constituencies and advance issue-based campaigns. This article reports findings from the research team's work with 21 peers sitting on the Labour frontbench. The researchers monitored and archived the peers' activity on Twitter for a period of 3 months between June and September 2012. Using a sample of 4,363 tweets and a mixed methodology combining semantic analysis, social network analysis, and quantitative analysis, this article explores the peers' patterns of usage and communication on Twitter. Key findings are that as a tweeting community their behavior is consistent with other communities. However, there is evidence that a coherent strategy is lacking in their coordinated use of the platform. Labour peers tend to work in small, clustered networks of self-interest as opposed to collectively to promote party policy.},
  langid = {english},
  keywords = {NLP,political_science,Social_media,Twitter},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\VZLJ96UK\Adi et al. - 2014 - Elite Tweets Analyzing the Twitter Communication .pdf}
}

@article{alencar2012,
  title = {Seeing beyond Reading: A Survey on Visual Text Analytics},
  shorttitle = {Seeing beyond Reading},
  author = {Alencar, Aretha B. and de Oliveira, Maria Cristina F. and Paulovich, Fernando V.},
  year = {2012},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {2},
  number = {6},
  pages = {476--492},
  issn = {1942-4795},
  doi = {10.1002/widm.1071},
  urldate = {2020-06-17},
  abstract = {We review recent visualization techniques aimed at supporting tasks that require the analysis of text documents, from approaches targeted at visually summarizing the relevant content of a single document to those aimed at assisting exploratory investigation of whole collections of documents.Techniques are organized considering their target input material---either single texts or collections of texts---and their focus, which may be at displaying content, emphasizing relevant relationships, highlighting the temporal evolution of a document or collection, or helping users to handle results from a query posed to a search engine.We describe the approaches adopted by distinct techniques and briefly review the strategies they employ to obtain meaningful text models, discuss how they extract the information required to produce representative visualizations, the tasks they intend to support and the interaction issues involved, and strengths and limitations. Finally, we show a summary of techniques, highlighting their goals and distinguishing characteristics. We also briefly discuss some open problems and research directions in the fields of visual text mining and text analytics. {\copyright} 2012 Wiley Periodicals, Inc. This article is categorized under: Algorithmic Development {$>$} Text Mining Technologies {$>$} Visualization},
  copyright = {Copyright {\copyright} 2012 John Wiley \& Sons, Inc.},
  langid = {english},
  keywords = {NLP,Quantitative_Analysis},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\QD4C6BN5\Alencar et al. - 2012 - Seeing beyond reading a survey on visual text ana.pdf}
}

@article{ambrosino2018,
  title = {What Topic Modeling Could Reveal about the Evolution of Economics},
  author = {Ambrosino, Angela and Cedrini, Mario and Davis, John B. and Fiori, Stefano and Guerzoni, Marco and Nuccio, Massimiliano},
  year = {2018},
  month = oct,
  journal = {Journal of Economic Methodology},
  volume = {25},
  number = {4},
  pages = {329--348},
  issn = {1350-178X},
  doi = {10.1080/1350178X.2018.1529215},
  urldate = {2018-11-06},
  abstract = {The paper presents the topic modeling technique known as Latent Dirichlet Allocation (LDA), a form of text-mining aiming at discovering the hidden (latent) thematic structure in large archives of documents. By applying LDA to the full text of the economics articles stored in the JSTOR database, we show how to construct a map of the discipline over time, and illustrate the potentialities of the technique for the study of the shifting structure of economics in a time of (possible) fragmentation.},
  keywords = {NLP,Quantitative_History,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\24YYYJ8N\Ambrosino et al_2018_What topic modeling could reveal about the evolution of economics.pdf}
}

@article{anastasopoulos2019,
  title = {Machine {{Learning}} for {{Public Administration Research}}, {{With Application}} to {{Organizational Reputation}}},
  author = {Anastasopoulos, L Jason and Whitford, Andrew B},
  year = {2019},
  month = jun,
  journal = {Journal of Public Administration Research and Theory},
  volume = {29},
  number = {3},
  pages = {491--510},
  issn = {1053-1858},
  doi = {10.1093/jopart/muy060},
  urldate = {2021-07-28},
  abstract = {Machine learning (ML) methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories of public administration and to develop new theories. Despite the excitement surrounding ML methods, clarity regarding their proper use and potential pitfalls is lacking. This article attempts to fill this gap in the literature through providing an ML ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of ML and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner. We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how ML techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies. All R code, analyses, and data described in this article can be found in the Supplementary Appendix.},
  keywords = {machine_learning,NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\VMNPTJ2Z\Anastasopoulos et Whitford - 2019 - Machine Learning for Public Administration Researc.pdf}
}

@article{arseniev-koehler2022a,
  title = {Machine {{Learning}} as a {{Model}} for {{Cultural Learning}}: {{Teaching}} an {{Algorithm What}} It {{Means}} to Be {{Fat}}},
  shorttitle = {Machine {{Learning}} as a {{Model}} for {{Cultural Learning}}},
  author = {{Arseniev-Koehler}, Alina and Foster, Jacob G.},
  year = {2022},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {51},
  number = {4},
  pages = {1484--1539},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/00491241221122603},
  urldate = {2022-12-21},
  abstract = {Public culture is a powerful source of cognitive socialization; for example, media language is full of meanings about body weight. Yet it remains unclear how individuals process meanings in public culture. We suggest that schema learning is a core mechanism by which public culture becomes personal culture. We propose that a burgeoning approach in computational text analysis ? neural word embeddings ? can be interpreted as a formal model for cultural learning. Embeddings allow us to empirically model schema learning and activation from natural language data. We illustrate our approach by extracting four lower-order schemas from news articles: the gender, moral, health, and class meanings of body weight. Using these lower-order schemas we quantify how words about body weight ?fill in the blanks? about gender, morality, health, and class. Our findings reinforce ongoing concerns that machine-learning models (e.g., of natural language) can encode and reproduce harmful human biases.},
  langid = {english},
  keywords = {cultural_learning,machine_learning,NLP,social_sciences_methodology,Sociology,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\KKNR89X6\Arseniev-Koehler et Foster - 2022 - Machine Learning as a Model for Cultural Learning.pdf}
}

@article{baele2019,
  title = {Conspiratorial {{Narratives}} in {{Violent Political Actors}}' {{Language}}},
  author = {Baele, Stephane J.},
  year = {2019},
  month = oct,
  journal = {Journal of Language and Social Psychology},
  volume = {38},
  number = {5-6},
  pages = {706--734},
  issn = {0261-927X, 1552-6526},
  doi = {10.1177/0261927X19868494},
  urldate = {2023-09-28},
  abstract = {This article articulates the concept of ``conspiratorial narratives''---defined as stories which integrate a large range of events and archetypal characters from past and present in a single teleological explanation for the alleged suffering of a given social group---and argues that this particular linguistic construct is a key marker of extremist language. Using three different cases to illustrate our theoretical contribution (Nazi propaganda, Rwandan genocidaires' radio, IS' messaging), we show that paying attention to conspiratorial narratives leads us to significantly revise classic accounts of violent actors' language, and provides a better understanding of the link between that language and violence itself---more precisely, why violence happens, how much violence is directed to whom, and when it occurs.},
  langid = {english},
  keywords = {International_relations,NLP,Terrorism},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\VC6U3CVF\Baele - 2019 - Conspiratorial Narratives in Violent Political Act.pdf}
}

@article{baele2021,
  title = {From ``{{Incel}}'' to ``{{Saint}}'': {{Analyzing}} the Violent Worldview behind the 2018 {{Toronto}} Attack},
  shorttitle = {From ``{{Incel}}'' to ``{{Saint}}''},
  author = {Baele, Stephane J. and Brace, Lewys and Coan, Travis G.},
  year = {2021},
  month = nov,
  journal = {Terrorism and Political Violence},
  volume = {33},
  number = {8},
  pages = {1667--1691},
  issn = {0954-6553, 1556-1836},
  doi = {10.1080/09546553.2019.1638256},
  urldate = {2023-09-28},
  abstract = {This paper combines qualitative and quantitative content analysis to map and analyze the ``Incel'' worldview shared by members of a misogynistic online community ideologically linked to several recent acts of politically motivated violence, including Alek Minassian's van attack in Toronto (2018) and Elliot Rodger's school shooting in Isla Vista (2014). Specifically, the paper analyses how support and motivation for violence results from the particular structure this worldview presents in terms of social categories and causal narratives.},
  langid = {english},
  keywords = {International_relations,NLP,Terrorism},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\MHN6HNTS\Baele et al. - 2021 - From “Incel” to “Saint” Analyzing the violent wor.pdf}
}

@article{baele2023,
  title = {Lethal {{Words}}: {{An Integrated Model}} of {{Violent Extremists}}' {{Language}}},
  shorttitle = {Lethal {{Words}}},
  author = {Baele, Stephane J. and Boyd, Katharine and Coan, Travis G. and Naserian, Elahe},
  year = {2023},
  month = may,
  journal = {Studies in Conflict \& Terrorism},
  pages = {1--26},
  issn = {1057-610X, 1521-0731},
  doi = {10.1080/1057610X.2023.2213963},
  urldate = {2023-09-28},
  abstract = {Violent extremists' communications regularly yield lethal consequences. Yet despite this importance, no clear framework yet exists that exposes in a coherent way the various markers of violent extremist language. We construct a theoretical model of violent extremists' language, integrating and refining existing approaches from different academic fields into a comprehensive account of its semantic content and structure. We evidence the empirical accuracy of this model by applying on the magazines of two ideologically different extremist groups (ISIS and US National-Socialist Movement) a bespoke computational text analysis method revealing the specific connections between the nouns, verbs, and adjectives of violent extremist corpora identified in our theoretical model.},
  langid = {english},
  keywords = {International_relations,NLP,Terrorism},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\IE5W4YWK\Baele et al. - 2023 - Lethal Words An Integrated Model of Violent Extre.pdf}
}

@book{baerg2020,
  title = {Crafting {{Consensus}}: {{Why Central Bankers Change Their Speech}} and {{How Speech Changes}} the {{Economy}}},
  shorttitle = {Crafting {{Consensus}}},
  author = {Baerg, Nicole},
  year = {2020},
  month = jul,
  publisher = {{Oxford University Press}},
  abstract = {In a world dependent on the constant sharing of information, central bankers increasingly communicate their policies to the mass public. Central bank communications are drafted in monetary policy committee meetings composed of policymakers with differing interests. Despite their differences, committee members must come together, write, and agree to an official policy statement. Once released to the public, central bank communications then affect citizens' actions and ultimately, the economy. But how exactly does this work? In Crafting Consensus, Nicole Baerg explains how the transparency of central bank communication depends on the configuration of committee members' preferences. Baerg argues that monetary policy committees composed of members with differing preferences over inflation are better suited to communicating precise information with the public. These diverse committees produce central bank statements of higher quality and less uncertainty than those from more homogeneous committees. Additionally, she argues that higher quality statements more effectively shape individuals' inflation expectations and move the economy in ways that policymakers intend. Baerg demonstrates that central bankers are not impartial technocrats and that their preferences and the institutional rules where they work matter for understanding the politics of monetary policy and variations in economic performance over time. Conducting empirical analysis from historical archival data, textual analysis, machine-learning, survey experiments, and cross-sectional time-series data, Crafting Consensus offers a new theory of committee decision making and a battery of empirical tests to provide a rich understanding of modern-day central banking.},
  googlebooks = {CyrtDwAAQBAJ},
  isbn = {978-0-19-049950-1},
  langid = {english},
  keywords = {Central_Bank_Communication,central_banking,expertise,NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\QQ65TBZI\Baerg_2020_Crafting Consensus.pdf}
}

@article{baier2018,
  title = {Academic {{Autonomy Beyond}} the {{Nation-State}}},
  author = {Baier, Christian and Gengnagel, Vincent},
  year = {2018},
  month = may,
  journal = {{\"O}sterreichische Zeitschrift f{\"u}r Soziologie},
  volume = {43},
  number = {1},
  pages = {65--92},
  issn = {1862-2585},
  doi = {10.1007/s11614-018-0297-7},
  urldate = {2022-11-22},
  abstract = {The social sciences and humanities (SSH) traditionally have a~close relationship to the nation-state and there are substantial disciplinary differences across countries. Drawing on Bourdieu's theory of the academic field, the present article examines how academic autonomy and heteronomy are applied as discursive strategies as the SSH compete for funding in the transnational European arena established by the European Research Council (ERC). To this end, we analyze mission statements of ERC Starting Grant projects in the SSH, using a~mixed methods approach of statistical text analysis (topic modeling) and qualitative content analysis. Although the ERC puts the SSH under the constraints of academic capitalism, the classical humanities secure a~strong position by signaling academic autonomy and engaging in the construction and consecration of European culture. However, economics and other social sciences gravitate toward a~more heteronomous self-representation, emphasizing the political and social utility of their research, while disciplines like neuro-science and psychology exhibit self-representations closely related to the ``hard sciences''---and relatively alien to the SSH.},
  langid = {english},
  keywords = {erc_institution,NLP,research_funding,Sociology_of_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\M4XKKR6I\Baier et Gengnagel - 2018 - Academic Autonomy Beyond the Nation-State.pdf}
}

@article{ban2019a,
  title = {How {{Newspapers Reveal Political Power}}},
  author = {Ban, Pamela and Fouirnaies, Alexander and Hall, Andrew B and Snyder, James M},
  year = {2019},
  month = oct,
  journal = {Political Science Research and Methods},
  volume = {7},
  number = {04},
  pages = {661--678},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2017.43},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\QYNFZ4VP\Ban et al. - 2019 - How Newspapers Reveal Political Power.pdf}
}

@article{barbera2021,
  title = {Automated Text Classification of News Articles: {{A}} Practical Guide},
  shorttitle = {Automated Text Classification of News Articles},
  author = {Barber{\'a}, Pablo and Boydstun, Amber E. and Linn, Suzanna and McMahon, Ryan and Nagler, Jonathan},
  year = {2021},
  journal = {Political Analysis},
  volume = {29},
  number = {1},
  pages = {19--42},
  publisher = {{Cambridge University Press}},
  keywords = {-Lu,Classification,machine_learning,NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\EXK6JZW7\Barberá et al. - 2021 - Automated text classification of news articles A .pdf}
}

@article{barron2018,
  title = {Individuals, Institutions, and Innovation in the Debates of the {{French Revolution}}},
  author = {Barron, Alexander T. J. and Huang, Jenny and Spang, Rebecca L. and DeDeo, Simon},
  year = {2018},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {18},
  pages = {4607--4612},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1717729115},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,NLP,Topic_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\7V9SAGVM\\pnas.1717729115.sapp.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\AU2HISV5\\Barron et al. - 2018 - Individuals, institutions, and innovation in the d.pdf}
}

@article{baumer2017,
  title = {Comparing Grounded Theory and Topic Modeling: {{Extreme}} Divergence or Unlikely Convergence?},
  shorttitle = {Comparing Grounded Theory and Topic Modeling},
  author = {Baumer, Eric P. S. and Mimno, David and Guha, Shion and Quan, Emily and Gay, Geri K.},
  year = {2017},
  journal = {Journal of the Association for Information Science and Technology},
  volume = {68},
  number = {6},
  pages = {1397--1410},
  issn = {2330-1643},
  doi = {10.1002/asi.23786},
  urldate = {2021-07-30},
  abstract = {Researchers in information science and related areas have developed various methods for analyzing textual data, such as survey responses. This article describes the application of analysis methods from two distinct fields, one method from interpretive social science and one method from statistical machine learning, to the same survey data. The results show that the two analyses produce some similar and some complementary insights about the phenomenon of interest, in this case, nonuse of social media. We compare both the processes of conducting these analyses and the results they produce to derive insights about each method's unique advantages and drawbacks, as well as the broader roles that these methods play in the respective fields where they are often used. These insights allow us to make more informed decisions about the tradeoffs in choosing different methods for analyzing textual data. Furthermore, this comparison suggests ways that such methods might be combined in novel and compelling ways.},
  langid = {english},
  keywords = {grounded_theory,NLP,Quantitative_Analysis,social_sciences_methodology},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\NCCMJR3L\Baumer et al. - 2017 - Comparing grounded theory and topic modeling Extr.pdf}
}

@article{baylis2018,
  title = {Weather Impacts Expressed Sentiment},
  author = {Baylis, Patrick and Obradovich, Nick and Kryvasheyeu, Yury and Chen, Haohui and Coviello, Lorenzo and Moro, Esteban and Cebrian, Manuel and Fowler, James H.},
  editor = {Preis, Tobias},
  year = {2018},
  month = apr,
  journal = {PLOS ONE},
  volume = {13},
  number = {4},
  pages = {e0195750},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0195750},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\MMF82XMW\Baylis et al. - 2018 - Weather impacts expressed sentiment.pdf}
}

@article{beauchamp2010,
  title = {Text-{{Based Scaling}} of {{Legislatures}}: {{A Comparison}} of {{Methods}} with {{Applications}} to the {{US Senate}} and {{UK House}} of {{Commons}}},
  author = {Beauchamp, Nick},
  year = {2010},
  pages = {30},
  abstract = {Many theories call for actors to occupy positions on a political spectrum, but although procedures for scaling legislatures with well-documented, poorly-disciplined voting are well established, scaling individuals in all other situations remains a challenge. A new approach is to use automated techniques to scale participants based not on their votes, but on written and spoken text. This paper compares five text-based scaling methods, some familiar from the political science literature, some not: three ``supervised'' approaches that use reference texts to score speakers, and two ``unsupervised'' approaches that scale without user input. Their theoretical foundations are examined and despite their apparent dissimilarities, the three supervised methods are shown both mathematically and via simulation to produce relatively similar results. The methods are then tested against a well-established scaling, DW-Nominate scores for the 2006 US Senate. Using the aggregate speech of all members of either party as two reference texts, the supervised approaches produce scores that closely correlate with DW-Nominate scores and with each other, while the unsupervised approaches do less well. When applied to a domain that cannot be scaled by votes alone, the UK House of Commons, the supervised approaches using aggregate party texts as references produce scalings that strongly separate members of the various parties. Furthermore, these scalings do endure across years, unless there has been a change in party control. In that case, removing party ministers and their abundant technical speech, or using more extreme or out-of-power parties, appears to alleviate the problem. Future users of automated text-based scaling may take from this a series of practical guidelines: that the supervised approaches seem to work best; that of these the Bayesian approach is more theoretically secure than the popular ``Wordscores,'' and the vector-projection method possibly more effective than either, but all three work similarly; and that when scaling legislatures, it works best to exclude leadership terminology or members, or to use two relatively extreme out-of-power parties. Having successfully replicated existing scales, we can also move on with greater confidence to the myriad other political dimensions accessible with properly chosen reference texts.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\7ULK8E48\Beauchamp - Text-Based Scaling of Legislatures A Comparison o.pdf}
}

@article{beil2002,
  title = {Frequent {{Term-Based Text Clustering}}},
  author = {Beil, Florian and Ester, Martin and Xu, Xiaowei},
  year = {2002},
  pages = {7},
  abstract = {Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-theart text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\JMM82JHV\Beil et al. - Frequent Term-Based Text Clustering.pdf}
}

@article{benoit2016,
  title = {Crowd-Sourced {{Text Analysis}}: {{Reproducible}} and {{Agile Production}} of {{Political Data}}},
  shorttitle = {Crowd-Sourced {{Text Analysis}}},
  author = {Benoit, Kenneth and Conway, Drew and Lauderdale, Benjamin E. and Laver, Michael and Mikhaylov, Slava},
  year = {2016},
  month = may,
  journal = {American Political Science Review},
  volume = {110},
  number = {2},
  pages = {278--295},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055416000058},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5B4T2FTV\Benoit et al. - 2016 - Crowd-sourced Text Analysis Reproducible and Agil.pdf}
}

@article{benoit2017,
  title = {Measuring and {{Explaining Political Sophistication Through Textual Complexity}}},
  author = {Benoit, Kenneth and Munger, Kevin and Spirling, Arthur},
  year = {2017},
  pages = {42},
  abstract = {The sophistication of political communication has been measured using ``readability'' scores developed from other contexts, but their application out of domain is problematic. We systematically review the shortcomings of previous measures, before developing a new approach, with software, better suited to the task. We use the crowd to perform thousands of pairwise comparisons of text snippets and incorporate these results into a statistical model of comprehension. We include previously excluded features such as parts of speech, and a measure of word rarity derived from term frequencies in the Google books dataset. Our technique not only shows which features are appropriate to the political domain and how, but also provides a measure easily applied and rescaled to political texts in a way that facilitates comparison with reference to a meaningful baseline. We reassess patterns in US and UK political corpora to demonstrate how substantive conclusions differ when using our improved approach.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\96CRHGIR\Benoit et al. - Measuring and Explaining Political Sophistication .pdf}
}

@article{blaydes2020,
  title = {Political Cultures: Measuring Values Heterogeneity},
  shorttitle = {Political Cultures},
  author = {Blaydes, Lisa and Grimmer, Justin},
  year = {2020},
  journal = {Political Science Research and Methods},
  volume = {8},
  number = {3},
  pages = {571--579},
  publisher = {{Cambridge University Press}},
  keywords = {NLP,political_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\T76NMY4Y\Blaydes et Grimmer - 2020 - Political cultures measuring values heterogeneity.pdf}
}

@article{bohr2020,
  title = {``{{Reporting}} on Climate Change: {{A}} Computational Analysis of {{U}}.{{S}}. Newspapers and Sources of Bias, 1997--2017''},
  shorttitle = {``{{Reporting}} on Climate Change},
  author = {Bohr, Jeremiah},
  year = {2020},
  month = mar,
  journal = {Global Environmental Change},
  volume = {61},
  pages = {102038},
  issn = {0959-3780},
  doi = {10.1016/j.gloenvcha.2020.102038},
  urldate = {2021-08-20},
  abstract = {News organizations constitute key sites of science communication between experts and lay audiences, giving many individuals their basic worldview of complex topics like climate change. Previous researchers have studied climate change news coverage to assess accuracy in reporting and potential sources of bias. These studies typically rely on manually coding articles from a handful of prestigious outlets, not allowing comparisons with smaller newspapers or providing enough diversity to assess the influence of partisan orientation or localized climate vulnerability on content production. Making these comparisons, this study indicates that partisan orientation, scale of circulation, and vulnerability to climate change correlate with several topics present in U.S. newspaper coverage of climate change. After assembling a corpus of over 78,000 articles covering two decades from 52~U.S. newspapers that are diverse in terms of geography, partisan orientation, scale of circulation, and objectively measured climate risk, a coherent set of latent topics were identified via an automated content analysis of climate change news coverage. Topic model results indicate that while outlet bias does not appear to impact the prevalence of coverage for most topics surrounding climate change, differences were evident for some topics based on partisan orientation, scale, or vulnerability status, particularly those relating to climate change denial, impacts, mitigation, or resource use. Overall, this paper provides a comprehensive study of U.S. newspaper coverage of climate change and identifies specific topics where outlet bias constitutes an important contextual factor.},
  langid = {english},
  keywords = {Climate_change,Newspapers,NLP,STM,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\AN8MCFYE\Bohr - 2020 - “Reporting on climate change A computational anal.pdf}
}

@article{bonica2014,
  title = {Mapping the {{Ideological Marketplace}}},
  author = {Bonica, Adam},
  year = {2014},
  journal = {American Journal of Political Science},
  volume = {58},
  number = {2},
  pages = {367--386},
  issn = {1540-5907},
  doi = {10.1111/ajps.12062},
  urldate = {2021-07-28},
  abstract = {I develop a method to measure the ideology of candidates and contributors using campaign finance data. Combined with a data set of over 100 million contribution records from state and federal elections, the method estimates ideal points for an expansive range of political actors. The common pool of contributors who give across institutions and levels of politics makes it possible to recover a unified set of ideological measures for members of Congress, the president and executive branch, state legislators, governors, and other state officials, as well as the interest groups and individuals who make political donations. Since candidates fundraise regardless of incumbency status, the method estimates ideal points for both incumbents and nonincumbents. After establishing measure validity and addressing issues concerning strategic behavior, I present results for a variety of political actors and discuss several promising avenues of research made possible by the new measures.},
  langid = {english},
  keywords = {Ideology,NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\GAVJ2GI5\Bonica - 2014 - Mapping the Ideological Marketplace.pdf}
}

@article{bonica2021,
  title = {Estimating {{Judicial Ideology}}},
  author = {Bonica, Adam and Sen, Maya},
  year = {2021},
  month = feb,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {1},
  pages = {97--118},
  issn = {0895-3309},
  doi = {10.1257/jep.35.1.97},
  urldate = {2021-07-28},
  abstract = {We review the substantial literature on estimating judicial ideology, from the US Supreme Court to the lowest state court. As a way to showcase the strengths and drawbacks of various measures, we further analyze trends in judicial polarization within the US federal courts. Our analysis shows substantial gaps in the ideology of judges appointed by Republican Presidents versus those appointed by Democrats. Similar to trends in Congressional polarization, the increasing gap is mostly driven by a rightward movement by judges appointed by Republicans. We conclude by noting important avenues for future research in the study of the ideology of judges.},
  langid = {english},
  keywords = {Ideology,NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\PRVKPCZH\Bonica et Sen - 2021 - Estimating Judicial Ideology.pdf}
}

@article{bonikowski2021,
  title = {The {{Partisan Sorting}} of ``{{America}}'': {{How Nationalist Cleavages Shaped}} the 2016 {{U}}.{{S}}. {{Presidential Election}}},
  shorttitle = {The {{Partisan Sorting}} of ``{{America}}''},
  author = {Bonikowski, Bart and Feinstein, Yuval and Bock, Sean},
  year = {2021},
  month = sep,
  journal = {American Journal of Sociology},
  volume = {127},
  number = {2},
  pages = {492--561},
  issn = {0002-9602, 1537-5390},
  doi = {10.1086/717103},
  urldate = {2023-04-30},
  langid = {english},
  keywords = {measuring_ideology,NLP,political_science,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\ME5V5QDZ\Bonikowski et al. - 2021 - The Partisan Sorting of “America” How Nationalist.pdf}
}

@article{bonikowski2022,
  title = {Politics as {{Usual}}? {{Measuring Populism}}, {{Nationalism}}, and {{Authoritarianism}} in {{U}}.{{S}}. {{Presidential Campaigns}} (1952--2020) with {{Neural Language Models}}},
  shorttitle = {Politics as {{Usual}}?},
  author = {Bonikowski, Bart and Luo, Yuchen and Stuhler, Oscar},
  year = {2022},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {51},
  number = {4},
  pages = {1721--1787},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/00491241221122317},
  urldate = {2023-04-30},
  abstract = {Radical-right campaigns commonly employ three discursive elements: anti-elite populism, exclusionary and declinist nationalism, and authoritarianism. Recent scholarship has explored whether these frames have diffused from radical-right to centrist parties in the latter's effort to compete for the former's voters. This study instead investigates whether similar frames had been used by mainstream political actors prior to their exploitation by the radical right (in the U.S., Donald Trump's 2016 and 2020 campaigns). To do so, we identify instances of populism, nationalism (i.e., exclusionary and inclusive definitions of national symbolic boundaries and displays of low and high national pride), and authoritarianism in the speeches of Democratic and Republican presidential nominees between 1952 and 2020. These frames are subtle, infrequent, and polysemic, which makes their measurement difficult. We overcome this by leveraging the affordances of neural language models---in particular, a robustly optimized variant of bidirectional encoder representations from Transformers (RoBERTa) and active learning. As we demonstrate, this approach is more effective for measuring discursive frames than other methods commonly used by social scientists. Our results suggest that what set Donald Trump's campaign apart from those of mainstream presidential candidates was not the invention of a new form of politics, but the combination of negative evaluations of elites, low national pride, and authoritarianism---all of which had long been present among both parties---with an explicit evocation of exclusionary nationalism, which had been articulated only implicitly by prior presidential nominees. Radical-right discourse---at least at the presidential level in the United States---should therefore be characterized not as a break with the past but as an amplification and creative rearrangement of existing political-cultural tropes.},
  langid = {english},
  keywords = {measuring_ideology,NLP,political_science,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\CPW8VBA9\Bonikowski et al. - 2022 - Politics as Usual Measuring Populism, Nationalism.pdf}
}

@article{bonikowski2022a,
  title = {Reclaiming the {{Past}} to {{Transcend}} the {{Present}}: {{Nostalgic Appeals}} in {{U}}.{{S}}. {{Presidential Elections}}},
  shorttitle = {Reclaiming the {{Past}} to {{Transcend}} the {{Present}}},
  author = {Bonikowski, Bart and Stuhler, Oscar},
  year = {2022},
  journal = {Sociological Forum},
  volume = {37},
  number = {S1},
  pages = {1263--1293},
  issn = {1573-7861},
  doi = {10.1111/socf.12838},
  urldate = {2023-10-18},
  abstract = {Nostalgic appeals to an idealized past are often employed in radical-right discourse. In this study, we examine precedents for this strategy in mainstream politics. We make use of recent advances in natural language processing---specifically Transformer-based neural language models and active learning---to identify instances of nostalgia in U.S. presidential campaign speeches from 1952 to 2020. We then ask what form nostalgia takes, when it has been most salient, what aspects of the nation it has been used to glorify, and how it relates to populist, nationalist, and authoritarian frames. Our findings demonstrate that nostalgic appeals tend not to involve rich descriptions of bygone historical periods, but instead take the form of brief and multivocal statements with a consistent lexical signature. Moreover, nostalgia is frequently used by challenger candidates from both parties to reinforce populist claims and expressions of low national pride. This points to discursive continuities between mainstream and radical-right actors. Where their respective messaging diverges is in the use of nostalgia to frame exclusionary nationalist and authoritarian claims, a practice limited to radical-right campaigns (in our data, those of Donald Trump). Rather than inventing their rhetorical strategies de novo, therefore, it appears that radical-right actors tend to adopt and creatively recombine frames already widespread in political culture.},
  copyright = {{\copyright} 2022 Eastern Sociological Society.},
  langid = {english},
  keywords = {_tablet,Discourse_analysis,machine_learning,NLP,NLP_reading_group,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\W2VI25PG\Bonikowski_Stuhler_2022_Reclaiming the Past to Transcend the Present.pdf}
}

@article{bruinsma2017,
  title = {Validating {{Wordscores}}},
  author = {Bruinsma, Bastiaan and Gemenis, Kostas},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.04737 [stat]},
  eprint = {1707.04737},
  primaryclass = {stat},
  urldate = {2020-09-22},
  abstract = {Wordscores is a popular quantitative text scaling method to estimate parties' positions on a priori specified dimensions, without requiring the researchers to read or even understand the language in the documents they are analysing. This study tries to establish whereas Wordscores is able to deliver this promise by conducting a rigorous validation of its output using the Euromanifestos of 164 parties across 23 countries. We assess content validity by looking at the scored words in their context, criterion validity by comparing the Wordscores output to expert surveys and other judgemental estimates of party positions, and construct validity by using the Wordscores estimates to predict party membership in the European Parliament groups. We conclude that, despite the promises, Wordscores fails to deliver valid party positions, and outline three conditions under which its performance can be improved.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\J77RCDWF\Bruinsma et Gemenis - 2017 - Validating Wordscores.pdf}
}

@article{catalinac2014,
  title = {Quantitative {{Text Analysis}} with {{Asian Languages}}: {{Some Problems}} and {{Solu-}} Tions},
  author = {Catalinac, Amy},
  year = {2014},
  number = {1},
  pages = {3},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\DBHPSLZ3\Catalinac - 2014 - Quantitative Text Analysis with Asian Languages S.pdf}
}

@article{catalinac2016,
  title = {From {{Pork}} to {{Policy}}: {{The Rise}} of {{Programmatic Campaigning}} in {{Japanese Elections}}},
  shorttitle = {From {{Pork}} to {{Policy}}},
  author = {Catalinac, Amy},
  year = {2016},
  month = jan,
  journal = {The Journal of Politics},
  volume = {78},
  number = {1},
  pages = {1--18},
  issn = {0022-3816, 1468-2508},
  doi = {10.1086/683073},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\YJEFXH8E\Catalinac - 2016 - From Pork to Policy The Rise of Programmatic Camp.pdf}
}

@article{catalinac2018,
  title = {Positioning under {{Alternative Electoral Systems}}: {{Evidence}} from {{Japanese Candidate Election Manifestos}}},
  shorttitle = {Positioning under {{Alternative Electoral Systems}}},
  author = {Catalinac, Amy},
  year = {2018},
  month = feb,
  journal = {American Political Science Review},
  volume = {112},
  number = {1},
  pages = {31--48},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055417000399},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\CZF7KMS3\Catalinac - 2018 - Positioning under Alternative Electoral Systems E.pdf}
}

@article{cointet2018,
  title = {{The impact of big data on sociological text analysis}},
  author = {Cointet, Jean-Philippe and Parasie, Sylvain and Garnier, Lucy},
  year = {2018},
  month = oct,
  journal = {Revue francaise de sociologie},
  volume = {59},
  number = {3},
  pages = {533--557},
  publisher = {{Presses de Sciences Po}},
  issn = {0035-2969},
  urldate = {2022-07-06},
  abstract = {Since the 2000s, new techniques for text analysis have appeared at the intersection of computer science, artificial intelligence, and natural language processing. While these technologies were designed independently of sociological concerns, they are now being used by researchers\&\#8212;sociologists and non-sociologists alike\&\#8212;with a view to renewing our knowledge of the social by drawing on the considerable volume of textual materials now available. By providing an overview of the sociological investigations that are based on the \&\#8220;datafication\&\#8221; and quantitative analysis of text corpora, this article identifies the conditions necessary for these approaches to constitute a resource for sociological inquiry. Three conditions emerge from our analysis: 1) knowledge of the context in which textual traces are produced; 2) the incorporation of extra-textual data into the inquiry; and 3) the adjustment of algorithms to sociological reasoning.},
  langid = {french},
  keywords = {Big_Data,NLP},
  annotation = {Bibliographie\_available: 1 Cairndomain: www.cairn-int.info Cite Par\_available: 0},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\59VBAPVY\Cointet et al. - 2018 - The impact of big data on sociological text analys.pdf}
}

@article{conti-browna,
  title = {Twitter and the {{Federal Reserve}}: {{How}} the {{U}}.{{S}}. Central Bank Is (and Is Not) Surviving Social Media},
  author = {{Conti-Brown}, Peter and Feinstein, Brian D},
  pages = {25},
  abstract = {In the last generation, the Federal Reserve launched a ``quiet revolution'' in how it approaches communication. Most of the tools that the Fed uses to communicate, however, have remained relatively static and indifferent to rapid technological change: speeches, congressional testimony, press conferences and select interviews, and the like. This white paper undertakes the first systematic analysis of the Fed's participation on Twitter, a medium of increasing importance to public policy discussions. We also analyze how President Donald Trump, one of the most prolific users of the medium, has influenced conversations about the Fed on Twitter.},
  langid = {english},
  keywords = {Central_Bank_Communication,central_banking,FED,NLP,Social_media,Twitter},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\SCM9Q7MG\Conti-Brown et Feinstein - Twitter and the Federal Reserve How the U.S. cent.pdf}
}

@book{curini2020,
  title = {The {{SAGE Handbook}} of {{Research Methods}} in {{Political Science}} and {{International Relations}}},
  author = {Curini, Luigi and Franzese, Robert},
  year = {2020},
  publisher = {{SAGE Publications Ltd}},
  address = {{1 Oliver's Yard,~55 City Road~London~EC1Y 1SP}},
  doi = {10.4135/9781526486387},
  urldate = {2021-07-28},
  isbn = {978-1-5264-5993-0 978-1-5297-7107-7},
  langid = {english},
  keywords = {Bayesian_statistics,Network_Analysis,NLP,political_science,Quantitative_Analysis,social_sciences_methodology},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\3RC4FKI4\Curini et Franzese - 2020 - The SAGE Handbook of Research Methods in Political.pdf}
}

@article{curran2018,
  title = {Look Who's Talking: {{Two-mode}} Networks as Representations of a Topic Model of {{New Zealand}} Parliamentary Speeches},
  shorttitle = {Look Who's Talking},
  author = {Curran, Ben and Higham, Kyle and Ortiz, Elisenda and Filho, Demival Vasques},
  year = {2018},
  month = jun,
  journal = {PLOS ONE},
  volume = {13},
  number = {6},
  pages = {e0199072},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0199072},
  urldate = {2020-09-25},
  abstract = {Quantitative methods to describe the participation to debate of Members of Parliament and the parties they belong to are lacking. Here we propose a new approach that combines topic modeling with complex networks techniques, and use it to characterize the political discourse at the New Zealand Parliament. We implement a Latent Dirichlet Allocation model to discover the thematic structure of the government's digital database of parliamentary speeches, and construct from it two-mode networks linking Members of the Parliament to the topics they discuss. Our results show how topic popularity changes over time and allow us to relate the trends followed by political parties in their discourses with specific social, economic and legislative events. Moreover, the community analysis of the two-mode network projections reveals which parties dominate the political debate as well as how much they tend to specialize in a small or large number of topics. Our work demonstrates the benefits of performing quantitative analysis in a domain normally reserved for qualitative approaches, providing an efficient way to measure political activity.},
  langid = {english},
  keywords = {Network_Analysis,NLP},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\43FGWDEA\\Curran et al. - 2018 - Look who’s talking Two-mode networks as represent.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\LQHX8DKE\\article.html}
}

@article{daubler2012,
  title = {Natural {{Sentences}} as {{Valid Units}} for {{Coded Political Texts}}},
  author = {D{\"a}ubler, Thomas and Benoit, Kenneth and Mikhaylov, Slava and Laver, Michael},
  year = {2012},
  month = oct,
  journal = {British Journal of Political Science},
  volume = {42},
  number = {4},
  pages = {937--951},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S0007123412000105},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\6IQSFLI7\Däubler et al. - 2012 - Natural Sentences as Valid Units for Coded Politic.pdf}
}

@article{denny2018,
  title = {Text {{Preprocessing For Unsupervised Learning}}: {{Why It Matters}}, {{When It Misleads}}, {{And What To Do About It}}},
  shorttitle = {Text {{Preprocessing For Unsupervised Learning}}},
  author = {Denny, Matthew J. and Spirling, Arthur},
  year = {2018},
  month = apr,
  journal = {Political Analysis},
  volume = {26},
  number = {2},
  pages = {168--189},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.44},
  urldate = {2020-09-22},
  abstract = {Despite the popularity of unsupervised techniques for political science text-as-data research, the importance and implications of preprocessing decisions in this domain have received scant systematic attention. Yet, as we show, such decisions have profound e ects on the results of real models for real data. We argue that substantive theory is typically too vague to be of use for feature selection, and that the supervised literature is not necessarily a helpful source of advice. To aid researchers working in unsupervised settings, we introduce a statistical procedure and so ware that examines the sensitivity of findings under alternate preprocessing regimes. This approach complements a researcher's substantive understanding of a problem by providing a characterization of the variability changes in preprocessing choices may induce when analyzing a particular dataset. In making scholars aware of the degree to which their results are likely to be sensitive to their preprocessing decisions, it aids replication e orts.},
  langid = {english},
  keywords = {-Lu,NLP,Preprocessing,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\D4PILD7U\Denny et Spirling - 2018 - Text Preprocessing For Unsupervised Learning Why .pdf}
}

@article{diessner2020,
  title = {Masters of the `Masters of the Universe'? {{Monetary}}, Fiscal and Financial Dominance in the {{Eurozone}}},
  shorttitle = {Masters of the `Masters of the Universe'?},
  author = {Diessner, Sebastian and Lisi, Giulio},
  year = {2020},
  month = apr,
  journal = {Socio-Economic Review},
  volume = {18},
  number = {2},
  pages = {315--335},
  issn = {1475-1461, 1475-147X},
  doi = {10.1093/ser/mwz017},
  urldate = {2021-07-28},
  abstract = {The rise of central bankers to the status of new `masters of the universe' has been matched by mounting allegations of political overreach. In the Eurozone, for instance, the European Central Bank has increasingly been accused of straying into the fiscal realm. Why do politically independent central banks engage intensely and publicly with government policies, thereby threatening the neat separation between monetary and fiscal policy that was meant to protect central banks themselves from interference? While existing political economy accounts have focused squarely on the issues of government debt and central bankers' fears of fiscal dominance, we argue for the emerging role of `financial dominance' throughout the crisis, thereby shedding light on the structural forces that master the new masters of the universe. To this end, we pursue a mixed-methods approach, combining quantitative text analysis techniques with a qualitative understanding of the context in which central banks communicate on fiscal policy.},
  langid = {english},
  keywords = {-Lu,Central_Bank_Communication,central_banking,NLP,political_science,Topic_modeling,UCL},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\G6XS7LLZ\Diessner et Lisi - 2020 - Masters of the ‘masters of the universe’ Monetary.pdf}
}

@article{do2022,
  title = {The {{Augmented Social Scientist}}: {{Using Sequential Transfer Learning}} to {{Annotate Millions}} of {{Texts}} with {{Human-Level Accuracy}}},
  shorttitle = {The {{Augmented Social Scientist}}},
  author = {Do, Salom{\'e} and Ollion, {\'E}tienne and Shen, Rubing},
  year = {2022},
  journal = {Sociological Methods \& Research},
  volume = {0},
  number = {0},
  pages = {1--34},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  keywords = {-Lu,machine_learning,NLP,social_sciences_methodology,Transformers},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\98FTU99N\Do et al_2022_The Augmented Social Scientist.pdf}
}

@article{dorazio2014,
  title = {Separating the {{Wheat}} from the {{Chaff}}: {{Applications}} of {{Automated Document Classification Using Support Vector Machines}}},
  shorttitle = {Separating the {{Wheat}} from the {{Chaff}}},
  author = {D'Orazio, Vito and Landis, Steven T. and Palmer, Glenn and Schrodt, Philip},
  year = {2014},
  journal = {Political Analysis},
  volume = {22},
  number = {2},
  pages = {224--242},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpt030},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,Classification,machine_learning,NLP,SVM_model},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\TGM2B9VI\D'Orazio et al. - 2014 - Separating the Wheat from the Chaff Applications .pdf}
}

@misc{egli2023,
  title = {Voting {{Booklet Bias}}: {{Stance Detection}} in {{Swiss Federal Communication}}},
  shorttitle = {Voting {{Booklet Bias}}},
  author = {Egli, Eric and Mami{\'e}, Noah and Dolev, Eyal Liron and M{\"u}ller, Mathias},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08999},
  eprint = {2306.08999},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.08999},
  urldate = {2023-11-15},
  abstract = {In this study, we use recent stance detection methods to study the stance (for, against or neutral) of statements in official information booklets for voters. Our main goal is to answer the fundamental question: are topics to be voted on presented in a neutral way? To this end, we first train and compare several models for stance detection on a large dataset about Swiss politics. We find that fine-tuning an M-BERT model leads to the best accuracy. We then use our best model to analyze the stance of utterances extracted from the Swiss federal voting booklet concerning the Swiss popular votes of September 2022, which is the main goal of this project. We evaluated the models in both a multilingual as well as a monolingual context for German, French, and Italian. Our analysis shows that some issues are heavily favored while others are more balanced, and that the results are largely consistent across languages. Our findings have implications for the editorial process of future voting booklets and the design of better automated systems for analyzing political discourse. The data and code accompanying this paper are available at https://github.com/ZurichNLP/voting-booklet-bias.},
  archiveprefix = {arxiv},
  keywords = {NLP,NLP_reading_group},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5AHIZPDS\Egli et al_2023_Voting Booklet Bias.pdf}
}

@article{eshbaugh-soha2010,
  title = {The Tone of Local Presidential News Coverage},
  author = {{Eshbaugh-Soha}, Matthew},
  year = {2010},
  journal = {Political Communication},
  volume = {27},
  number = {2},
  pages = {121--140},
  publisher = {{Taylor \& Francis}},
  keywords = {NLP,political_science,sentiment_analysis},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\L5I69XNM\Eshbaugh-Soha - 2010 - The tone of local presidential news coverage.pdf}
}

@article{evans2007,
  title = {Recounting the {{Courts}}? {{Applying Automated Content Analysis}} to {{Enhance Empirical Legal Research}}: {{Automated Content Analysis}} to {{Enhance Empirical Legal Research}}},
  shorttitle = {Recounting the {{Courts}}?},
  author = {Evans, Michael and McIntosh, Wayne and Lin, Jimmy and Cates, Cynthia},
  year = {2007},
  month = dec,
  journal = {Journal of Empirical Legal Studies},
  volume = {4},
  number = {4},
  pages = {1007--1039},
  issn = {17401453},
  doi = {10.1111/j.1740-1461.2007.00113.x},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\CJAUEQSS\Evans et al. - 2007 - Recounting the Courts Applying Automated Content .pdf}
}

@article{evans2016,
  title = {Machine Translation: Mining Text for Social Theory},
  shorttitle = {Machine Translation},
  author = {Evans, James A. and Aceves, Pedro},
  year = {2016},
  journal = {Annual Review of Sociology},
  volume = {42},
  pages = {21--50},
  publisher = {{Annual Reviews}},
  keywords = {content_analysis_methods,grounded_theory,NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\M2G36YXR\Evans et Aceves - 2016 - Machine translation mining text for social theory.pdf}
}

@article{ferrara2021,
  title = {Exports vs. Investment: {{How}} Political Discourse Shapes Popular Support for External Imbalances},
  shorttitle = {Exports vs. Investment},
  author = {Ferrara, Federico Maria and Stefan Haas, J{\"o}rg and Peterson, Andrew and Sattler, Thomas},
  year = {2021},
  month = feb,
  journal = {Socio-Economic Review},
  pages = {mwab004},
  issn = {1475-1461, 1475-147X},
  doi = {10.1093/ser/mwab004},
  urldate = {2021-08-20},
  abstract = {The economic imbalances that characterize the world economy have unequally distributed costs and benefits. That raises the question how countries could run long-term external surpluses and deficits without significant opposition against the policies that generate them. We show that political discourse helps to secure public support for these policies and the resulting economic outcomes. First, a content analysis of 32,000 newspaper articles finds that the dominant interpretations of current account balances in Australia and Germany concur with very distinct perspectives: external surpluses are seen as evidence of competitiveness in Germany, while external deficits are interpreted as evidence of attractiveness for investments in Australia. Second, survey experiments in both countries suggest that exposure to these diverging interpretations has a causal effect on citizens' support for their country's economic strategy. Political discourse, thus, is crucial to provide the societal foundation of national growth strategies.},
  langid = {english},
  keywords = {External_imbalances,International_Macroeconomics,Newspapers,NLP,political_science,STM,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\YYLCTPPZ\Ferrara et al. - 2021 - Exports vs. investment How political discourse sh.pdf}
}

@misc{field2018,
  title = {Framing and {{Agenda-setting}} in {{Russian News}}: A {{Computational Analysis}} of {{Intricate Political Strategies}}},
  shorttitle = {Framing and {{Agenda-setting}} in {{Russian News}}},
  author = {Field, Anjalie and Kliger, Doron and Wintner, Shuly and Pan, Jennifer and Jurafsky, Dan and Tsvetkov, Yulia},
  year = {2018},
  month = oct,
  number = {arXiv:1808.09386},
  eprint = {1808.09386},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.09386},
  urldate = {2023-11-08},
  abstract = {Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and "fake news'". Here, we draw on two concepts from the political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.},
  archiveprefix = {arxiv},
  keywords = {-Lu,BERT,Climate_change,Framing,NLP,Transformers},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\YRE84RGM\Field et al_2018_Framing and Agenda-setting in Russian News.pdf}
}

@article{fligstein2017,
  title = {Seeing {{Like}} the {{Fed}}: {{Culture}}, {{Cognition}}, and {{Framing}} in the {{Failure}} to {{Anticipate}} the {{Financial Crisis}} of 2008},
  shorttitle = {Seeing {{Like}} the {{Fed}}},
  author = {Fligstein, Neil and Stuart Brundage, Jonah and Schultz, Michael},
  year = {2017},
  month = oct,
  journal = {American Sociological Review},
  volume = {82},
  number = {5},
  pages = {879--909},
  issn = {0003-1224},
  doi = {10.1177/0003122417728240},
  urldate = {2020-02-10},
  abstract = {One of the puzzles about the financial crisis of 2008 is why regulators, particularly the Federal Open Market Committee (FOMC), were so slow to recognize the impending collapse of the financial system and its broader consequences for the economy. We use theory from the literature on culture, cognition, and framing to explain this puzzle. Consistent with recent work on ``positive asymmetry,'' we show how the FOMC generally interpreted discomforting facts in a positive light, marginalizing and normalizing anomalous information. We argue that all frames limit what can be understood, but the content of frames matters for how facts are identified and explained. We provide evidence that the Federal Reserve's primary frame for making sense of the economy was macroeconomic theory. The content of macroeconomics made it difficult for the FOMC to connect events into a narrative reflecting the links between foreclosures in the housing market, the financial instruments used to package the mortgages into securities, and the threats to the larger economy. We conclude with implications for the sociological literatures on framing and cognition and for decision-making in future crises.},
  langid = {english},
  keywords = {-Lu,central_banking,Fed,Financial_crisis,Framing,Macroeconomics_and_Finance,NLP,Rebuilding,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\NQB3GJUD\Fligstein et al. - 2017 - Seeing Like the Fed Culture, Cognition, and Frami.pdf}
}

@article{garg2018,
  title = {Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes},
  author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
  year = {2018},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {16},
  pages = {E3635-E3644},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1720347115},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,gender_biases,Gender_Studies,NLP,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\ERQQEA3D\Garg et al. - 2018 - Word embeddings quantify 100 years of gender and e.pdf}
}

@techreport{gentzkow2016,
  title = {Measuring {{Polarization}} in {{High-Dimensional Data}}: {{Method}} and {{Application}} to {{Congressional Speech}}},
  shorttitle = {Measuring {{Polarization}} in {{High-Dimensional Data}}},
  author = {Gentzkow, Matthew and Shapiro, Jesse and Taddy, Matt},
  year = {2016},
  month = jul,
  number = {id:11114},
  institution = {{eSocialSciences}},
  urldate = {2019-05-09},
  abstract = {This paper studies trends in the partisanship of Congressional speech from 1873 to 2009. It defines partisanship to be the ease with which an observer could infer a congressperson{\^a}{\texteuro}™s party from a fixed amount of speech, and estimates it using a structural choice model and methods from machine learning. This paper applies tools from structural estimation and machine learning to study the partisanship of language in the US Congress. [Working Paper 22423]},
  langid = {english},
  keywords = {NLP,polarization,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\I36PC32A\Gentzkow et al. - 2016 - Measuring Polarization in High-Dimensional Data M.pdf}
}

@article{gorodnichenko,
  title = {`{{Liked}}', `{{Shared}}', `{{Commented}}': {{Central Bank Communication}} on {{Facebook}} and {{Twitter}}},
  author = {Gorodnichenko, Yuriy and Pham, Tho and Talavera, Oleksandr},
  pages = {41},
  abstract = {This study is a comprehensive analysis of the Federal Reserve System (FED) communication on social media and its effectiveness. Our examination shows that although the FED uses both Twitter and Facebook for public outreach, communication via Twitter is more popular and gains greater public engagement. There are heterogeneous effects across different topics of the FED's social media posts, post types, as well as across Twitter user groups. The general public is most active in engaging with the FED accounts, followed by media, investors, academics, and government accounts. Further investigation suggests inconclusive evidence of stock market reactions to the FED communication on social media. However, market participants do update their inflation expectations based on information contained in the FED's social media posts.},
  langid = {english},
  keywords = {Central_Bank_Communication,central_banking,NLP,Social_media,Twitter},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\Z3I7NVJD\Gorodnichenko et al. - ‘Liked’, ‘Shared’, ‘Commented’ Central Bank Commu.pdf}
}

@article{greene2016,
  title = {Competing on the Issues: {{How}} Experience in Government and Economic Conditions Influence the Scope of Parties' Policy Messages},
  shorttitle = {Competing on the Issues},
  author = {Greene, Zachary},
  year = {2016},
  journal = {Party Politics},
  volume = {22},
  number = {6},
  pages = {809--822},
  publisher = {{SAGE Publications Sage UK: London, England}},
  keywords = {NLP,political_science,text_diversity},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\VRPKHCTA\Greene - 2016 - Competing on the issues How experience in governm.pdf}
}

@article{greene2017,
  title = {Working through the Issues: How Issue Diversity and Ideological Disagreement Influence Coalition Duration},
  shorttitle = {Working through the Issues},
  author = {Greene, Zachary},
  year = {2017},
  journal = {European Political Science Review},
  volume = {9},
  number = {4},
  pages = {561--585},
  publisher = {{Cambridge University Press}},
  keywords = {NLP,political_science,text_diversity},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\Q733MPYX\Greene - 2017 - Working through the issues how issue diversity an.pdf}
}

@article{greene2017a,
  title = {Exploring the {{Political Agenda}} of the {{European Parliament Using}} a {{Dynamic Topic Modeling Approach}}},
  author = {Greene, Derek and Cross, James P.},
  year = {2017},
  month = jan,
  journal = {Political Analysis},
  volume = {25},
  number = {1},
  pages = {77--94},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2016.7},
  urldate = {2023-10-18},
  abstract = {This study analyzes the political agenda of the European Parliament (EP) plenary, how it has evolved over time, and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making plenary speeches. To unveil the plenary agenda and detect latent themes in legislative speeches over time, MEP speech content is analyzed using a new dynamic topic modeling method based on two layers of Non-negative Matrix Factorization (NMF). This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999 to 2014. Our findings suggest that two-layer NMF is a valuable alternative to existing dynamic topic modeling approaches found in the literature, and can unveil niche topics and associated vocabularies not captured by existing methods. Substantively, our findings suggest that the political agenda of the EP evolves significantly over time and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro Crisis. MEP contributions to the plenary agenda are also found to be impacted upon by voting behavior and the committee structure of the Parliament.},
  langid = {english},
  keywords = {Discourse_analysis,dynamic_topic_modeling,European_parliament,NLP,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\X8TW2Y74\Greene_Cross_2017_Exploring the Political Agenda of the European Parliament Using a Dynamic Topic.pdf}
}

@article{grimmer2011,
  title = {General Purpose Computer-Assisted Clustering and Conceptualization},
  author = {Grimmer, J. and King, G.},
  year = {2011},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {7},
  pages = {2643--2650},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1018067108},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\732IMWMN\Grimmer et King - 2011 - General purpose computer-assisted clustering and c.pdf}
}

@article{grimmer2013,
  title = {Text as {{Data}}: {{The Promise}} and {{Pitfalls}} of {{Automatic Content Analysis Methods}} for {{Political Texts}}},
  shorttitle = {Text as {{Data}}},
  author = {Grimmer, Justin and Stewart, Brandon M.},
  year = {2013},
  journal = {Political Analysis},
  volume = {21},
  number = {3},
  pages = {267--297},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mps028},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,Classification,machine_learning,NLP,political_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\4D2SHMUR\Grimmer et Stewart - 2013 - Text as Data The Promise and Pitfalls of Automati.pdf}
}

@article{grimmer2015,
  title = {The {{Unreliability}} of {{Measures}} of {{Intercoder Reliability}}, and {{What}} to Do {{About}} It},
  author = {Grimmer, Justin and King, Gary and Superti, Chiara},
  year = {2015},
  pages = {36},
  abstract = {In both automated and traditional text analysis, human coders are regularly tasked with categorizing documents. Researchers then evaluate the success of this crucial step in the research process via one of many measures of intercoder reliability, such as Cronbachs alpha. They then improve coding practices until this measure reaches some arbitrary threshold, at which point remaining disagreements are resolved in arbitrary ways and ignored in subsequent analyses. We show that this common practice can generate severely biased estimates and misleading conclusions. The problem is the focus on measures of intercoder reliability which, except at the extreme, are unrelated to the quantities of interest, such as the proportion of documents in each category. We thus develop an approach that enables scholars to directly incorporate coding uncertainty into statistical estimation. The method offers an interval estimate which we prove contains the true proportion of documents in each category, under reasonable assumptions. We then extend this method to situations with multiple coders, when one coder is trusted more than another, and when the resulting document codes are used as inputs to another statistical model. We offer easy-to-use software that implements all our suggestions.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\LNWECXL5\Grimmer et al. - The Unreliability of Measures of Intercoder Reliab.pdf}
}

@article{hengel2017,
  title = {Publishing While Female},
  author = {Hengel, Erin},
  year = {2017},
  pages = {72},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\4R9TZJSB\Hengel - Publishing while female.pdf}
}

@article{himelboim2013,
  title = {Tweeting {{Apart}}: {{Applying Network Analysis}} to {{Detect Selective Exposure Clusters}} in {{Twitter}}},
  shorttitle = {Tweeting {{Apart}}},
  author = {Himelboim, Itai and Smith, Marc and Shneiderman, Ben},
  year = {2013},
  month = oct,
  journal = {Communication Methods and Measures},
  volume = {7},
  number = {3-4},
  pages = {195--223},
  issn = {1931-2458, 1931-2466},
  doi = {10.1080/19312458.2013.813922},
  urldate = {2022-05-23},
  langid = {english},
  keywords = {Central_Bank_Communication,central_banking,Network_Analysis,NLP,Social_media,Twitter},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\SC7293HX\Himelboim et al. - 2013 - Tweeting Apart Applying Network Analysis to Detec.pdf}
}

@article{hjorth2015,
  title = {Computers, Coders, and Voters: {{Comparing}} Automated Methods for Estimating Party Positions},
  shorttitle = {Computers, Coders, and Voters},
  author = {Hjorth, Frederik and Klemmensen, Robert and Hobolt, Sara and Hansen, Martin Ejnar and {Kurrild-Klitgaard}, Peter},
  year = {2015},
  month = jun,
  journal = {Research \& Politics},
  volume = {2},
  number = {2},
  pages = {205316801558047},
  issn = {2053-1680, 2053-1680},
  doi = {10.1177/2053168015580476},
  urldate = {2020-09-22},
  abstract = {Assigning political actors positions in ideological space is a task of key importance to political scientists. In this paper we compare estimates obtained using the automated Wordscores and Wordfish techniques, along with estimates from voters and the Comparative Manifesto Project (CMP), against expert placements. We estimate the positions of 254 manifestos across 33 elections in Germany and Denmark, two cases with very different textual data available. We find that Wordscores approximately replicates the CMP, voter, and expert assessments of party positions in both cases, whereas Wordfish replicates the positions in the German manifestos only. The results demonstrate that automated methods can produce valid estimates of party positions, but also that the appropriateness of each method hinges on the quality of the textual data. Additional analyses suggest that Wordfish requires both longer texts and a more ideologically charged vocabulary in order to produce estimates comparable to Wordscores. The paper contributes to the literature on automated content analysis by providing a comprehensive test of convergent validation, in terms of both number of cases analyzed and number of validation measures.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\FNQ9HWBQ\Hjorth et al. - 2015 - Computers, coders, and voters Comparing automated.pdf}
}

@article{hofstra2020,
  title = {The {{Diversity}}--{{Innovation Paradox}} in {{Science}}},
  author = {Hofstra, Bas and Kulkarni, Vivek V. and {Munoz-Najar Galvez}, Sebastian and He, Bryan and Jurafsky, Dan and McFarland, Daniel A.},
  year = {2020},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {17},
  pages = {9284--9291},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1915378117},
  urldate = {2021-08-01},
  abstract = {Prior work finds a diversity paradox: Diversity breeds innovation, yet underrepresented groups that diversify organizations have less successful careers within them. Does the diversity paradox hold for scientists as well? We study this by utilizing a near-complete population of {$\sim$}1.2 million US doctoral recipients from 1977 to 2015 and following their careers into publishing and faculty positions. We use text analysis and machine learning to answer a series of questions: How do we detect scientific innovations? Are underrepresented groups more likely to generate scientific innovations? And are the innovations of underrepresented groups adopted and rewarded? Our analyses show that underrepresented groups produce higher rates of scientific novelty. However, their novel contributions are devalued and discounted: For example, novel contributions by gender and racial minorities are taken up by other scholars at lower rates than novel contributions by gender and racial majorities, and equally impactful contributions of gender and racial minorities are less likely to result in successful scientific careers than for majority groups. These results suggest there may be unwarranted reproduction of stratification in academic careers that discounts diversity's role in innovation and partly explains the underrepresentation of some groups in academia.},
  langid = {english},
  keywords = {-Lu,innovation,NLP,PhD,scientometrics},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\IMEB3KE9\\Hofstra et al. - 2020 - The Diversity–Innovation Paradox in Science.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\SVINE3H2\\pnas.1915378117.sapp.pdf}
}

@article{hsu2021,
  title = {Narrative Economics Using Textual Analysis of Newspaper Data: New Insights into the {{U}}.{{S}}. {{Silver Purchase Act}} and {{Chinese}} Price Level in 1928--1936},
  shorttitle = {Narrative Economics Using Textual Analysis of Newspaper Data},
  author = {Hsu, Ching and Yu, Tina and Chen, Shu-Heng},
  year = {2021},
  month = nov,
  journal = {Journal of Computational Social Science},
  volume = {4},
  number = {2},
  pages = {761--785},
  issn = {2432-2725},
  doi = {10.1007/s42001-021-00104-0},
  urldate = {2024-01-03},
  abstract = {In light of the recent advancement in economic narrative analysis, we develop a computational textual analysis method to study economic history. In this method, we collect narrative data from newspapers to measure economic trends. In particular, the popularity (frequency) of a narrative (keyword) on the newspapers is used as the proxy of the amount of economic activities associated with the narrative term; a high frequency indicates that there is a high volume of economic activities associated with the narrative term and vice versa. Regularized regression algorithms are then applied on the narrative frequency data to identify narrative terms whose associated microeconomic activities have macroeconomic impact. We apply the method to study a classic topic in Chinese economic history research: U.S. Silver Purchase Act and the Chinese price level in 1928--1936. Our results provide new insights into this controversial subject. For example, we find that the economic activity associated with the narrative term silver stock had no impact on the Chinese price level, which is contrary to previous research on the topic by Friedman and Schwartz [10]. Meanwhile, economic activities associated with the narrative terms U.S. silver purchase act and silver export are found to have a negative impact on the Chinese price level. This suggests the concerns at that time about the effects of U.S. Silver Purchase Act on the Chinese economy were not misplaced.},
  langid = {english},
  keywords = {Narrative_economics,NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\Z6BEDFBJ\Hsu et al_2021_Narrative economics using textual analysis of newspaper data.pdf}
}

@article{iaria2018,
  title = {Frontier {{Knowledge}} and {{Scientific Production}}: {{Evidence}} from the {{Collapse}} of {{International Science}}*},
  shorttitle = {Frontier {{Knowledge}} and {{Scientific Production}}},
  author = {Iaria, Alessandro and Schwarz, Carlo and Waldinger, Fabian},
  year = {2018},
  month = may,
  journal = {The Quarterly Journal of Economics},
  volume = {133},
  number = {2},
  pages = {927--991},
  issn = {0033-5533},
  doi = {10.1093/qje/qjx046},
  urldate = {2023-05-19},
  abstract = {We show that World War I and the subsequent boycott against Central scientists severely interrupted international scientific cooperation. After 1914, citations to recent research from abroad decreased and paper titles became less similar (evaluated by latent semantic analysis), suggesting a reduction in international knowledge flows. Reduced international scientific cooperation led to a decline in the production of basic science and its application in new technology. Specifically, we compare productivity changes for scientists who relied on frontier research from abroad, to changes for scientists who relied on frontier research from home. After 1914, scientists who relied on frontier research from abroad published fewer papers in top scientific journals, produced less Nobel Prize--nominated research, introduced fewer novel scientific words, and introduced fewer novel words that appeared in the text of subsequent patent grants. The productivity of scientists who relied on top 1\% research declined twice as much as the productivity of scientists who relied on top 3\% research. Furthermore, highly prolific scientists experienced the starkest absolute productivity declines. This suggests that access to the very best research is key for scientific and technological progress.},
  keywords = {NLP,scientometrics},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\NU3KQGF5\Iaria et al. - 2018 - Frontier Knowledge and Scientific Production Evid.pdf}
}

@article{ibrocevic2018,
  title = {All {{Economic Ideas}} Are {{Equal}}, but {{Some}} Are More {{Equal}} than {{Others}}: {{A Differentiated Perspective}} on {{Macroprudential Ideas}} and {{Their Implementation}}},
  shorttitle = {All {{Economic Ideas}} Are {{Equal}}, but {{Some}} Are More {{Equal}} than {{Others}}},
  author = {Ibrocevic, Edin and Thiemann, Matthias},
  year = {2018},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3195910},
  urldate = {2019-08-07},
  langid = {english},
  keywords = {-Lu,central_banking,Macroprudential policy,NLP,Rebuilding,Topic_modeling,UCL}
}

@article{isoaho,
  title = {Topic Modelling and Text Analysis for Qualitative Policy Research},
  author = {Isoaho, Karoliina and Gritsenko, Daria and M{\"a}kel{\"a}, Eetu},
  pages = {50},
  abstract = {This paper contributes to a critical methodological discussion that has direct ramifications for policy studies: how computational methods can be concretely incorporated into existing processes of textual analysis and interpretation without compromising scientific integrity. We focus on the computational method of topic modelling and investigate how it interacts with two larger families of qualitative methods: content and classification methods characterised by interest in words as communication units and discourse and representation methods characterised by interest in the meaning of communicative acts. Based on analysis of recent academic publications that have used topic modelling for textual analysis, our findings show that different mixed-method research designs are appropriate when combining TM with the two groups of methods. Our main concluding argument is that topic modelling enables scholars to apply policy theories and concepts to much larger sets of data. That said, the use of computational methods requires genuine understanding of these techniques to obtain substantially meaningful results. We encourage policy scholars to reflect carefully on methodological issues, and offer a simple heuristic to help identify and address critical points when designing a study using topic modelling.},
  langid = {english},
  keywords = {NLP,political_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\TQEI4TQ7\Isoaho et al. - Topic modelling and text analysis for qualitative .pdf}
}

@article{jacobi2016,
  title = {Quantitative Analysis of Large Amounts of Journalistic Texts Using Topic Modelling},
  author = {Jacobi, Carina and Van Atteveldt, Wouter and Welbers, Kasper},
  year = {2016},
  journal = {Digital journalism},
  volume = {4},
  number = {1},
  pages = {89--106},
  publisher = {{Taylor \& Francis}},
  keywords = {Newspapers,NLP,political_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\2FI3DSUE\Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf}
}

@article{jaros2018,
  title = {China's {{Newsmakers}}: {{Official Media Coverage}} and {{Political Shifts}} in the {{Xi Jinping Era}}},
  shorttitle = {China's {{Newsmakers}}},
  author = {Jaros, Kyle and Pan, Jennifer},
  year = {2018},
  month = mar,
  journal = {The China Quarterly},
  volume = {233},
  pages = {111--136},
  issn = {0305-7410, 1468-2648},
  doi = {10.1017/S0305741017001679},
  urldate = {2020-09-22},
  abstract = {Xi Jinping's rise to power in late 2012 brought immediate political realignments in China, but the extent of these shifts has remained unclear. In this paper, we evaluate whether the perceived changes associated with Xi Jinping's ascent -- increased personalization of power, centralization of authority, Party dominance and anti-Western sentiment -- were reflected in the content of provincial-level official media. As past research makes clear, media in China have strong signalling functions, and media coverage patterns can reveal which actors are up and down in politics. Applying innovations in automated text analysis to nearly two million newspaper articles published between 2011 and 2014, we identify and tabulate the individuals and organizations appearing in official media coverage in order to help characterize political shifts in the early years of Xi Jinping's leadership. We find substantively mixed and regionally varied trends in the media coverage of political actors, qualifying the prevailing picture of China's ``new normal.'' Provincial media coverage reflects increases in the personalization and centralization of political authority, but we find a drop in the media profile of Party organizations and see uneven declines in the media profile of foreign actors. More generally, we highlight marked variation across provinces in coverage trends.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\RAFM5PA7\Jaros et Pan - 2018 - China's Newsmakers Official Media Coverage and Po.pdf}
}

@book{jockers2013,
  title = {Macroanalysis: Digital Methods and Literary History},
  shorttitle = {Macroanalysis},
  author = {Jockers, Matthew Lee},
  year = {2013},
  series = {Topics in the Digital Humanities},
  publisher = {{University of Illinois Press}},
  address = {{Urbana}},
  isbn = {978-0-252-03752-8 978-0-252-07907-8 978-0-252-09476-7},
  langid = {english},
  lccn = {PN73 .J63 2013},
  keywords = {NLP,Quantitative_Analysis},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5TYERQCI\Jockers - 2013 - Macroanalysis digital methods and literary histor.pdf}
}

@article{johnson2019,
  title = {Adding Rooms onto a House We Love: {{Central}} Banking after the Global Financial Crisis},
  shorttitle = {Adding Rooms onto a House We Love},
  author = {Johnson, Juliet and {Arel-Bundock}, Vincent and Portniaguine, Vladislav},
  year = {2019},
  journal = {Public Administration},
  volume = {97},
  number = {3},
  pages = {546--560},
  publisher = {{Wiley Online Library}},
  keywords = {-Lu,Central_Bank,Central_Bank_Independence,Great_Recession,Macroprudential policy,NLP,political_science,Rebuilding,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\EQVCNVQG\Johnson et al. - 2019 - Adding rooms onto a house we love Central banking.pdf}
}

@article{karell2019,
  title = {Rhetorics of {{Radicalism}}},
  author = {Karell, Daniel and Freedman, Michael},
  year = {2019},
  pages = {63},
  langid = {english},
  keywords = {NLP,political_science,Radicalism,Rhetoric,Sociology_of_networks},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\26THBGPS\Karell et Freedman - 2019 - Rhetorics of Radicalism.pdf}
}

@article{king2003a,
  title = {An {{Automated Information Extraction Tool}} for {{International Conflict Data}} with {{Performance}} as {{Good}} as {{Human Coders}}: {{A Rare Events Evaluation Design}}},
  shorttitle = {An {{Automated Information Extraction Tool}} for {{International Conflict Data}} with {{Performance}} as {{Good}} as {{Human Coders}}},
  author = {King, Gary and Lowe, Will},
  year = {2003},
  journal = {International Organization},
  volume = {57},
  number = {3},
  pages = {617--642},
  issn = {0020-8183, 1531-5088},
  doi = {10.1017/S0020818303573064},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP,scaling_method,wordscore_model},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\7PQWKVMK\King et Lowe - 2003 - An Automated Information Extraction Tool for Inter.pdf}
}

@article{kluver2015,
  title = {Measuring Interest Group Framing Strategies in Public Policy Debates},
  author = {Kl{\"u}ver, Heike and Mahoney, Christine},
  year = {2015},
  month = aug,
  journal = {Journal of Public Policy},
  volume = {35},
  number = {2},
  pages = {223--244},
  issn = {0143-814X, 1469-7815},
  doi = {10.1017/S0143814X14000294},
  urldate = {2020-09-22},
  abstract = {Framing plays an important role in lobbying, as interest groups strategically highlight some aspects of policy proposals while ignoring others to shape policy debates in their favour. However, due to methodological difficulties, we have remarkably little systematic data about the framing strategies of interest groups. This article therefore proposes a new technique for measuring interest group framing that is based on a quantitative text analysis of interest group position papers and official policy documents. We test this novel methodological approach on the basis of two case studies in the areas of environmental and transport policy in the European Union. We are able to identify the frames employed by all interest groups mobilised in a debate and assess their effectiveness by studying to what extent decision-makers move closer to their policy positions over the course of the policy debate.},
  langid = {english},
  keywords = {-Lu,Framing,Lobbying,NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\Y482JH8M\Klüver et Mahoney - 2015 - Measuring interest group framing strategies in pub.pdf}
}

@article{korhonen,
  title = {Mastering {{Central Bank Communication Challenges}} via {{Twitter}}},
  author = {Korhonen, Iikka and Newby, Elisa},
  pages = {22},
  abstract = {This study examines the Twitter policies and use of European central banks. Almost every European central bank maintains an institutional Twitter account, but tweeting activity, tweet content and usage restrictions on Twitter use by individual staff members vary considerably. We further consider the evolution of Twitter use by European central banks in light of the growing importance of financial stability in central bank policy messaging. To study these issues, we create a database of tweets from European central banks and financial supervisors, as well as attempt to gauge how closely professional economists follow central banks on Twitter. Central banks' Twitter activity has no relation to citizens' online participation. We also find that central banks' communication on financial stability with Twitter has increased over time, especially in comparison with monetary policy.},
  langid = {english},
  keywords = {Central_Bank_Communication,central_banking,NLP,Social_media,Twitter},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\8XJWI33K\Korhonen et Newby - Mastering Central Bank Communication Challenges vi.pdf}
}

@article{kozlowski2022,
  title = {Intersectional Inequalities in Science},
  author = {Kozlowski, Diego and Larivi{\`e}re, Vincent and Sugimoto, Cassidy R. and {Monroe-White}, Thema},
  year = {2022},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {2},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2113067119},
  urldate = {2022-01-08},
  abstract = {The US scientific workforce is primarily composed of White men. Studies have demonstrated the systemic barriers preventing women and other minoritized populations from gaining entry to science; few, however, have taken an intersectional perspective and examined the consequences of these inequalities on scientific knowledge. We provide a large-scale bibliometric analysis of the relationship between intersectional identities, topics, and scientific impact. We find homophily between identities and topic, suggesting a relationship between diversity in the scientific workforce and expansion of the knowledge base. However, topic selection comes at a cost to minoritized individuals for whom we observe both between- and within-topic citation disadvantages. To enhance the robustness of science, research organizations should provide adequate resources to historically underfunded research areas while simultaneously providing access for minoritized individuals into high-prestige networks and topics.},
  chapter = {Social Sciences},
  copyright = {{\copyright} 2021 . https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {34983876},
  keywords = {bibliometrics,Inequality,NLP,scientometrics,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\DXWYVWLB\Kozlowski et al. - 2022 - Intersectional inequalities in science.pdf}
}

@article{kraft2018,
  title = {Measuring {{Morality}} in {{Political Attitude Expression}}},
  author = {Kraft, Patrick W.},
  year = {2018},
  month = jul,
  journal = {The Journal of Politics},
  volume = {80},
  number = {3},
  pages = {1028--1033},
  issn = {0022-3816, 1468-2508},
  doi = {10.1086/696862},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\AJFH3R6R\Kraft - 2018 - Measuring Morality in Political Attitude Expressio.pdf}
}

@article{kutter2020,
  title = {Construction of the {{Eurozone}} Crisis: Re- and Depoliticising {{European}} Economic Integration},
  shorttitle = {Construction of the {{Eurozone}} Crisis},
  author = {Kutter, Amelie},
  year = {2020},
  month = jul,
  journal = {Journal of European Integration},
  volume = {42},
  number = {5},
  pages = {659--676},
  issn = {0703-6337, 1477-2280},
  doi = {10.1080/07036337.2020.1792466},
  urldate = {2022-04-19},
  abstract = {The Eurozone crisis is among recent developments that upset the European Union (EU) most profoundly. It deprived large parts of the population of their previous standard of living, put the rationality of the Economic and Monetary Union (EMU) into doubt and sparked unprecedented contestation. This article adopts a discursive notion of politicisation and the frame of Discursive Political Studies to investigate whether that moment of contestation re-politicised EU economic governance in substantive terms. It argues that, while emerging counter-narratives of crisis projected alternative scenar\- ios of economic integration and established a practice of construc\- tive EU critique, they were co-opted by the dominant massmediated story of a public debt crisis. This is shown in a combined content, network and discourse analysis of crisis narra\- tives, which were issued by leading EU representatives, antiausterity parties and European media between 2010 and 2013.},
  langid = {english},
  keywords = {Discursive_Analysis,Euro_crisis,NLP,political_science,Politicisation/depoliticisation},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\VDK97QPQ\Kutter - 2020 - Construction of the Eurozone crisis re- and depol.pdf}
}

@article{lauderdale2016,
  title = {Measuring {{Political Positions}} from {{Legislative Speech}}},
  author = {Lauderdale, Benjamin E. and Herzog, Alexander},
  year = {2016},
  journal = {Political Analysis},
  volume = {24},
  number = {3},
  pages = {374--394},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpw017},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,NLP,quanteda,wordfish_model},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\FSZID72Y\Lauderdale et Herzog - 2016 - Measuring Political Positions from Legislative Spe.pdf}
}

@article{laver2003,
  title = {Extracting {{Policy Positions}} from {{Political Texts Using Words}} as {{Data}}},
  author = {Laver, Michael and Benoit, Kenneth and Garry, John},
  year = {2003},
  journal = {The American Political Science Review},
  volume = {97},
  number = {2},
  eprint = {3118211},
  eprinttype = {jstor},
  pages = {311--331},
  langid = {english},
  keywords = {Ideology,NLP,political_science,wordscore_model},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\HUMTX5KF\\Laver et al. - 2003 - Extracting Policy Positions from Political Texts U.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\VU2PVHUZ\\Laver et al. - 2003 - Extracting Policy Positions from Political Texts U.pdf}
}

@article{li2023,
  title = {``{{There}}'s {{No Data Like More Data}}'': {{Automatic Speech Recognition}} and the {{Making}} of {{Algorithmic Culture}}},
  shorttitle = {``{{There}}'s {{No Data Like More Data}}''},
  author = {Li, Xiaochang},
  year = {2023},
  month = jul,
  journal = {Osiris},
  volume = {38},
  pages = {165--182},
  publisher = {{The University of Chicago Press}},
  issn = {0369-7827},
  doi = {10.1086/725132},
  urldate = {2023-07-28},
  abstract = {This article examines the role of automatic speech recognition research in the rise of data-driven machine learning as a privileged and pervasive form of computational knowledge. It focuses on IBM's Continuous Speech Recognition group between 1972 and 1993 as they fueled speech recognition's ``statistical turn,'' uprooting the field from the simulation of human reason and language understanding and redirecting it toward the acquisition of data for large-scale pattern recognition. This shift, I argue, was instrumental in the remaking of artificial intelligence and computational modeling into radically data-centric pursuits that underpin algorithmic culture today. In doing so, this history offers a critical piece in the story of how we became data-driven, highlighting how efforts to turn language into data consequently turned data into an imperative, preparing the way for the widespread incursion of algorithmic authority across everyday life.},
  keywords = {algorithms,machine_learning,NLP,Quantification,science_studies,Sociology_of_quantification},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\9QTH4WYK\Li_2023_“There’s No Data Like More Data”.pdf}
}

@article{lowe2008,
  title = {Understanding {{Wordscores}}},
  author = {Lowe, Will},
  year = {2008},
  journal = {Political Analysis},
  volume = {16},
  number = {4},
  pages = {356--371},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpn004},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP,scaling_method,wordscore_model},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\CSCWB2MS\Lowe - 2008 - Understanding Wordscores.pdf}
}

@article{lucas2015,
  title = {Computer-{{Assisted Text Analysis}} for {{Comparative Politics}}},
  author = {Lucas, Christopher and Nielsen, Richard A. and Roberts, Margaret E. and Stewart, Brandon M. and Storer, Alex and Tingley, Dustin},
  year = {2015},
  journal = {Political Analysis},
  volume = {23},
  number = {2},
  pages = {254--277},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpu019},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5UBAFGDY\Lucas et al. - 2015 - Computer-Assisted Text Analysis for Comparative Po.pdf}
}

@article{lundberg2022,
  title = {Researcher Reasoning Meets Computational Capacity: {{Machine}} Learning for Social Science},
  shorttitle = {Researcher Reasoning Meets Computational Capacity},
  author = {Lundberg, Ian and Brand, Jennie E. and Jeon, Nanum},
  year = {2022},
  month = nov,
  journal = {Social Science Research},
  volume = {108},
  pages = {102807},
  issn = {0049-089X},
  doi = {10.1016/j.ssresearch.2022.102807},
  urldate = {2023-04-30},
  abstract = {Computational power and big data have created new opportunities to explore and understand the social world. A special synergy is possible when social scientists combine human attention to certain aspects of the problem with the power of algorithms to automate other aspects of the problem. We review selected exemplary applications where machine learning amplifies researcher coding, summarizes complex data, relaxes statistical assumptions, and targets researcher attention to further social science research. We aim to reduce perceived barriers to machine learning by summarizing several fundamental building blocks and their grounding in classical statistics. We present a few guiding principles and promising approaches where we see particular potential for machine learning to transform social science inquiry. We conclude that machine learning tools are increasingly accessible, worthy of attention, and ready to yield new discoveries for social research.},
  langid = {english},
  keywords = {_tablet,machine_learning,NLP,social_sciences_methodology,survey},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\XLKKP9FV\Lundberg et al_2022_Researcher reasoning meets computational capacity.pdf}
}

@misc{luo2021,
  title = {Detecting {{Stance}} in {{Media}} on {{Global Warming}}},
  author = {Luo, Yiwei and Card, Dallas and Jurafsky, Dan},
  year = {2021},
  month = jan,
  number = {arXiv:2010.15149},
  eprint = {2010.15149},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.15149},
  urldate = {2023-11-08},
  abstract = {Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, "Leading scientists agree that global warming is a serious concern," framing a clause which affirms their own stance ("that global warming is serious") as an opinion endorsed ("[scientists] agree") by a reputable source ("leading"). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: "Mistaken scientists claim [...]." Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce Global Warming Stance Dataset (GWSD), a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other's opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author's own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.},
  archiveprefix = {arxiv},
  keywords = {-Lu,attributing_opinion,BERT,Climate_change,Framing,NLP,Transformers},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\YQTXBM8Y\Luo et al_2021_Detecting Stance in Media on Global Warming.pdf}
}

@article{macanovic2022,
  title = {Text Mining for Social Science -- {{The}} State and the Future of Computational Text Analysis in Sociology},
  author = {Macanovic, Ana},
  year = {2022},
  month = nov,
  journal = {Social Science Research},
  volume = {108},
  pages = {102784},
  issn = {0049-089X},
  doi = {10.1016/j.ssresearch.2022.102784},
  urldate = {2023-04-30},
  abstract = {The emergence of big data and computational tools has introduced new possibilities for using large-scale textual sources in sociological research. Recent work in sociology of culture, science, and economic sociology has shown how computational text analysis can be used in theory building and testing. This review starts with an introduction of the history of computer-assisted text analysis in sociology and then proceeds to discuss five families of computational methods used in contemporary research. Using exemplary studies, it shows how dictionary methods, semantic and network analysis tools, language models, unsupervised, and supervised machine learning can assist sociologists with different analytical tasks. After presenting recent methodological developments, this review summarizes several important implications of using large datasets and computational methods to infer complex meaning in texts. Finally, it calls researchers from different methodological traditions to adopt text mining tools while remaining mindful of lessons learned from working with conventional data and methods.},
  langid = {english},
  keywords = {_tablet,machine_learning,NLP,social_sciences_methodology,survey,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\SSJP3JIK\Macanovic_2022_Text mining for social science – The state and the future of computational text.pdf}
}

@book{maesse2021,
  title = {Power and {{Influence}} of {{Economists}}: {{Contributions}} to the {{Social Studies}} of {{Economics}}},
  shorttitle = {Power and {{Influence}} of {{Economists}}},
  editor = {Maesse, Jens and P{\"u}hringer, Stephan and Rossier, Thierry and Benz, Pierre},
  year = {2021},
  month = may,
  publisher = {{Routledge}},
  address = {{London}},
  doi = {10.4324/9780367817084},
  abstract = {Economists occupy leading positions in many different sectors including central and private banks, multinational corporations, the state and the media, as well as serving as policy consultants on everything from health to the environment and security. Power and Influence of Economists explores the interconnected relationship between power, knowledge and influence which has led economics to be both a source and beneficiary of widespread power and influence.  The contributors to this book explore the complex and diverse methods and channels that economists have used to exert and expand their influence from different disciplinary and national perspectives. Four different analytical views on the role of power and economics are taken: first, the role of economic expert discourses as power devices for the formation of influential expertise; second, the logics and modalities of governmentality that produce power/knowledge apparatuses between science and society; third, economists as involved in networks between academia, politics and the media; and forth, economics considered as a social field, including questions of legitimacy and unequal relations between economists based on the detention of various capitals. The volume includes case studies on a variety of national configurations of economics, such as the US, Germany, Italy, Switzerland, Greece, Mexico and Brazil, as well as international spaces and organisations such as the IMF.  This book provides innovative research perspectives for students and scholars of heterodox economics, cultural political economy, sociology of professions, network studies, and the social studies of power, discourse and knowledge. ``The Open Access version of this book, available at https://www.taylorfrancis.com/books/oa-edit/10.4324/9780367817084, has been made available under a Creative Commons Attribution-Non Commercial-No Derivatives 4.0 license.''},
  isbn = {978-0-367-81708-4},
  keywords = {expertise,NLP,Sociology_of_economics},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\X2NLUUT8\Maesse et al. - 2021 - Power and Influence of Economists Contributions t.pdf}
}

@article{malaterre2019,
  title = {What Is This Thing Called {{Philosophy}} of {{Science}}? {{A}} Computational Topic-Modeling Perspective, 1934--2015},
  shorttitle = {What Is This Thing Called {{Philosophy}} of {{Science}}?},
  author = {Malaterre, Christophe and Chartier, Jean-Fran{\c c}ois and Pulizzotto, Davide},
  year = {2019},
  journal = {HOPOS: The Journal of the International Society for the History of Philosophy of Science},
  volume = {9},
  number = {2},
  pages = {215--249},
  publisher = {{The University of Chicago Press Chicago, IL}},
  keywords = {-Lu,NLP,Topic_modeling}
}

@inproceedings{mendelsohn2021,
  title = {Modeling {{Framing}} in {{Immigration Discourse}} on {{Social Media}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Mendelsohn, Julia and Budak, Ceren and Jurgens, David},
  year = {2021},
  pages = {2219--2263},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.179},
  urldate = {2022-07-12},
  langid = {english},
  keywords = {Framing,machine_learning,NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5RVU7Q6E\Mendelsohn et al. - 2021 - Modeling Framing in Immigration Discourse on Socia.pdf}
}

@book{milner2015,
  title = {Sailing the Water's Edge: The Domestic Politics of {{American}} Foreign Policy},
  shorttitle = {Sailing the Water's Edge},
  author = {Milner, Helen V. and Tingley, Dustin},
  year = {2015},
  publisher = {{Princeton University Press}},
  address = {{Princeton, New Jersey}},
  abstract = {"When engaging with other countries, the U.S. government has a number of different policy instruments at its disposal, including foreign aid, international trade, and the use of military force. But what determines which policies are chosen? Does the United States rely too much on the use of military power and coercion in its foreign policies? Sailing the Water's Edge focuses on how domestic U.S. politics--in particular the interactions between the president, Congress, interest groups, bureaucratic institutions, and the public--have influenced foreign policy choices since World War II and shows why presidents have more control over some policy instruments than others. Presidential power matters and it varies systematically across policy instruments. Helen Milner and Dustin Tingley consider how Congress and interest groups have substantial material interests in and ideological divisions around certain issues and that these factors constrain presidents from applying specific tools. As a result, presidents select instruments that they have more control over, such as use of the military. This militarization of U.S. foreign policy raises concerns about the nature of American engagement, substitution among policy tools, and the future of U.S. foreign policy. Milner and Tingley explore whether American foreign policy will remain guided by a grand strategy of liberal internationalism, what affects American foreign policy successes and failures, and the role of U.S. intelligence collection in shaping foreign policy. The authors support their arguments with rigorous theorizing, quantitative analysis, and focused case studies, such as U.S. foreign policy in Sub-Saharan Africa across two presidential administrations. Sailing the Water's Edge examines the importance of domestic political coalitions and institutions on the formation of American foreign policy. "--},
  isbn = {978-0-691-16547-9},
  langid = {english},
  lccn = {JZ1480 .M555 2015},
  keywords = {Framing,NLP,political_science,STM},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\I6KLLNP9\Milner et Tingley - 2015 - Sailing the water's edge the domestic politics of.pdf}
}

@article{mohr2013,
  title = {Introduction---{{Topic}} Models: {{What}} They Are and Why They Matter},
  shorttitle = {Introduction---{{Topic}} Models},
  author = {Mohr, John W. and Bogdanov, Petko},
  year = {2013},
  month = dec,
  journal = {Poetics},
  series = {Topic {{Models}} and the {{Cultural Sciences}}},
  volume = {41},
  number = {6},
  pages = {545--569},
  issn = {0304-422X},
  doi = {10.1016/j.poetic.2013.10.001},
  urldate = {2021-01-02},
  abstract = {We provide a brief, non-technical introduction to the text mining methodology known as ``topic modeling.'' We summarize the theory and background of the method and discuss what kinds of things are found by topic models. Using a text corpus comprised of the eight articles from the special issue of Poetics on the subject of topic models, we run a topic model on these articles, both as a way to introduce the methodology and also to help summarize some of the ways in which social and cultural scientists are using topic models. We review some of the critiques and debates over the use of the method and finally, we link these developments back to some of the original innovations in the field of content analysis that were pioneered by Harold D. Lasswell and colleagues during and just after World War II.},
  langid = {english},
  keywords = {NLP,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\FCHXQNJS\Mohr_Bogdanov_2013_Introduction—Topic models.pdf}
}

@article{monroe2008,
  title = {Fightin' {{Words}}: {{Lexical Feature Selection}} and {{Evaluation}} for {{Identifying}} the {{Content}} of {{Political Conflict}}},
  shorttitle = {Fightin' {{Words}}},
  author = {Monroe, Burt L. and Colaresi, Michael P. and Quinn, Kevin M.},
  year = {2008},
  journal = {Political Analysis},
  volume = {16},
  number = {4},
  pages = {372--403},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpn018},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {dictionary_methods,NLP,political_science,Preprocessing},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\UDL4QHBN\Monroe et al. - 2008 - Fightin' Words Lexical Feature Selection and Eval.pdf}
}

@article{moschella2019,
  title = {Central Banks' Communication as Reputation Management: {{How}} the {{Fed}} Talks under Uncertainty},
  shorttitle = {Central Banks' Communication as Reputation Management},
  author = {Moschella, Manuela and Pinto, Luca},
  year = {2019},
  journal = {Public Administration},
  volume = {97},
  number = {3},
  pages = {513--529},
  issn = {1467-9299},
  doi = {10.1111/padm.12543},
  urldate = {2020-01-28},
  abstract = {This article advances a reputation-based account to explain the relative salience that different issues assume in central banks' communication. Based on an innovative dataset consisting of a corpus of speeches by the members of the Board of Governors of the Federal Reserve System of the United States (also known as the Fed) delivered from 2006 to 2016, the analysis shows that the most salient issues in the Fed's communication are shaped by reputational concerns about policy reversals. Specifically, when these concerns are higher, the Fed is more likely to focus on issues related to areas where its reputation is weak or not yet established---that is, issues related to credit easing and systemic financial regulation. In contrast, issues related to activities where the Fed's reputation is established are likely to become less salient---that is, issues related to economic activity and inflation. A similar pattern of issue attention is observed when the Fed addresses political audiences compared to other audiences.},
  langid = {english},
  keywords = {Central_Bank_Communication,central_banking,Fed,NLP,Political_economy,political_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\PTD5HKE3\Moschella et Pinto - 2019 - Central banks’ communication as reputation managem.pdf}
}

@article{moschella2020,
  title = {Let's Speak More? {{How}} the {{ECB}} Responds to Public Contestation},
  shorttitle = {Let's Speak More?},
  author = {Moschella, Manuela and Pinto, Luca and Martocchia Diodati, Nicola},
  year = {2020},
  journal = {Journal of European Public Policy},
  volume = {27},
  number = {3},
  pages = {400--418},
  publisher = {{Taylor \& Francis}},
  keywords = {-Lu,Central_Bank_Communication,central_banking,ECB,NLP,political_science,Politicisation/depoliticisation,STM,Topic_modeling,UCL},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\9LI3F4H8\\Moschella et al. - 2020 - Let's speak more How the ECB responds to public c.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\UHP6X7CX\\rjpp_a_1712457_sm9628.docx}
}

@article{mourlon-druol2021,
  title = {Talking about {{Europe}}: Exploring 70 Years of News Archives {\textbar} {{Bruegel}}},
  shorttitle = {Talking about {{Europe}}},
  author = {{Mourlon-Druol}, Emmanuel and Bergamini, Enrico},
  year = {2021},
  journal = {Working Paper Bruegel},
  urldate = {2021-12-14},
  abstract = {This paper aims to contribute to the understanding of Europe as reflected in European media.},
  langid = {american},
  keywords = {Europe_History,European_Union,Newspapers,NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\BK8KKR35\Mourlon-Druol et Bergamini - 2021 - Talking about Europe exploring 70 years of news a.pdf}
}

@article{munger2019,
  title = {Elites {{Tweet}} to {{Get Feet Off}} the {{Streets}}: {{Measuring Regime Social Media Strategies During Protest}}},
  shorttitle = {Elites {{Tweet}} to {{Get Feet Off}} the {{Streets}}},
  author = {Munger, Kevin and Bonneau, Richard and Nagler, Jonathan and Tucker, Joshua A.},
  year = {2019},
  month = oct,
  journal = {Political Science Research and Methods},
  volume = {7},
  number = {04},
  pages = {815--834},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2018.3},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\XIX93JH3\Munger et al. - 2019 - Elites Tweet to Get Feet Off the Streets Measurin.pdf}
}

@article{nelson2020,
  title = {Computational {{Grounded Theory}}: {{A Methodological Framework}}},
  shorttitle = {Computational {{Grounded Theory}}},
  author = {Nelson, Laura K.},
  year = {2020},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {49},
  number = {1},
  pages = {3--42},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117729703},
  urldate = {2021-07-30},
  abstract = {This article proposes a three-step methodological framework called computational grounded theory, which combines expert human knowledge and hermeneutic skills with the processing power and pattern recognition of computers, producing a more methodologically rigorous but interpretive approach to content analysis. The first, pattern detection step, involves inductive computational exploration of text, using techniques such as unsupervised machine learning and word scores to help researchers to see novel patterns in their data. The second, pattern refinement step, returns to an interpretive engagement with the data through qualitative deep reading or further exploration of the data. The third, pattern confirmation step, assesses the inductively identified patterns using further computational and natural language processing techniques. The result is an efficient, rigorous, and fully reproducible computational grounded theory. This framework can be applied to any qualitative text as data, including transcribed speeches, interviews, open-ended survey data, or ethnographic field notes, and can address many potential research questions.},
  langid = {english},
  keywords = {-Lu,grounded_theory,NLP,Quantitative_Analysis,social_sciences_methodology},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\N5ZNPEV7\Nelson - 2020 - Computational Grounded Theory A Methodological Fr.pdf}
}

@article{ollion2023,
  title = {Chatgpt for {{Text Annotation}}? {{Mind}} the {{Hype}}!},
  shorttitle = {Chatgpt for {{Text Annotation}}?},
  author = {Ollion, Etienne and Shen, Rubing and Macanovic, Ana and Chatelain, Arnault},
  year = {2023},
  publisher = {{SocArXiv}},
  urldate = {2023-10-18},
  keywords = {_tablet,chatGPT,machine_learning,NLP,social_sciences_methodology,text_annotation},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\TW5TF2L8\Ollion et al_2023_Chatgpt for Text Annotation.pdf}
}

@article{pansardi,
  title = {A ``{{More Political}}'' {{Commission}}? {{Reassessing EC Politicization}} through {{Language}}},
  shorttitle = {A ``{{More Political}}'' {{Commission}}?},
  author = {Pansardi, Pamela and Tortola, Pier Domenico},
  journal = {JCMS: Journal of Common Market Studies},
  volume = {n/a},
  number = {n/a},
  issn = {1468-5965},
  doi = {10.1111/jcms.13298},
  urldate = {2022-04-14},
  abstract = {This article contributes to the study of the European Commission's (EC) politicization by examining this phenomenon from the angle of communication. We elaborate a novel approach based on two linguistic indicators -- charisma and technicality -- which we then apply through a content analysis of 8,947 speeches delivered by Commission members between 1999 and 2019. Contrary to the narrative of an ever more political Commission, we find that the linguistic politicization of the EC decreased over the period under exam, reaching its nadir during Jean-Claude Juncker's presidential term (2014--19). Our findings raise the question of whether language is yet another ordinary dimension of politicization, or rather it is used strategically by the Commission to underplay its underlying politicization as measured in more traditional institutional, policy, and individual terms. Either way, our study highlights the multi-faceted nature of the politicization concept, and the need for deeper and more nuanced analyses of it.},
  langid = {english},
  keywords = {European_Commission,European_Studies,NLP,Politicisation/depoliticisation},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\ZZ82DUPQ\Pansardi et Tortola - A “More Political” Commission Reassessing EC Poli.pdf}
}

@article{paolacerchiello2018,
  title = {Assessing {{News Contagion}} in {{Finance}}},
  author = {{Paola Cerchiello} and {Giancarlo Nicola}},
  year = {2018},
  month = feb,
  journal = {Econometrics},
  volume = {6},
  number = {1},
  pages = {5},
  issn = {2225-1146},
  doi = {10.3390/econometrics6010005},
  urldate = {2020-09-22},
  abstract = {The analysis of news in the financial context has gained a prominent interest in the last years. This is because of the possible predictive power of such content especially in terms of associated sentiment/mood. In this paper, we focus on a specific aspect of financial news analysis: how the covered topics modify according to space and time dimensions. To this purpose, we employ a modified version of topic model LDA, the so-called Structural Topic Model (STM), that takes into account covariates as well. Our aim is to study the possible evolution of topics extracted from two well known news archive---Reuters and Bloomberg---and to investigate a causal effect in the diffusion of the news by means of a Granger causality test. Our results show that both the temporal dynamics and the spatial differentiation matter in the news contagion.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\LJFZGC6K\Paola Cerchiello et Giancarlo Nicola - 2018 - Assessing News Contagion in Finance.pdf}
}

@article{pattison2022,
  title = {The Devil We Know and the Angel That Did Not Fly: {{An}} Examination of Devil/Angel Shift in Twitter Fracking ``Debates'' in {{NY}} 2008--2018},
  shorttitle = {The Devil We Know and the Angel That Did Not Fly},
  author = {Pattison, Andrew and Cipolli, William and Marichal, Jose},
  year = {2022},
  month = jan,
  journal = {Review of Policy Research},
  volume = {39},
  number = {1},
  pages = {51--72},
  issn = {1541-132X, 1541-1338},
  doi = {10.1111/ropr.12452},
  urldate = {2022-05-23},
  langid = {english},
  keywords = {NLP,Social_media,Twitter},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\SRQUVI9E\Pattison et al. - 2022 - The devil we know and the angel that did not fly .pdf}
}

@article{pauwels2011,
  title = {Measuring {{Populism}}: {{A Quantitative Text Analysis}} of {{Party Literature}} in {{Belgium}}},
  shorttitle = {Measuring {{Populism}}},
  author = {Pauwels, Teun},
  year = {2011},
  month = feb,
  journal = {Journal of Elections, Public Opinion \& Parties},
  volume = {21},
  number = {1},
  pages = {97--119},
  issn = {1745-7289, 1745-7297},
  doi = {10.1080/17457289.2011.539483},
  urldate = {2020-09-22},
  abstract = {Despite the increased use of the term populism in vernacular and scholarly language, the measurement of the concept has long been neglected. The label is often attached to a certain party without any justification. Minimal definitions are alternatives but lack rigor. Classical content analyses provide more systematic measurements of populism but are extremely resource hungry. This article proposes an alternative, quantitative text analysis to measure the degree of populism among Belgian parties, drawing on both internally and externally oriented party literature. The results confirm that usual suspects such as the Vlaams Belang or Lijst Dedecker are the most populist of all parties under study. Populism turns out not to be an ``either--or'' concept, however, since we also identify a moderately populist party. It is furthermore demonstrated how populism can be attached to other ideologies, such as the radical right and (neo)liberalism. A cross-validation of the proposed method with independent voter survey data confirms its validity. This article concludes that a quantitative text analysis might be a promising method to measure populism over time and space without the huge costs of hand coding.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\94BSR5VV\Pauwels - 2011 - Measuring Populism A Quantitative Text Analysis o.pdf}
}

@misc{pennebaker2007,
  title = {Computerized {{Text Analysis}} of {{Al-Qaeda Transcripts}}},
  author = {Pennebaker and Chung},
  year = {2007},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\VYZZB8Y4\Pennebaker et Chung - 2007 - Computerized Text Analysis of Al-Qaeda Transcripts.pdf}
}

@article{peterson2018,
  title = {Classification {{Accuracy}} as a {{Substantive Quantity}} of {{Interest}}: {{Measuring Polarization}} in {{Westminster Systems}}},
  shorttitle = {Classification {{Accuracy}} as a {{Substantive Quantity}} of {{Interest}}},
  author = {Peterson, Andrew and Spirling, Arthur},
  year = {2018},
  month = jan,
  journal = {Political Analysis},
  volume = {26},
  number = {1},
  pages = {120--128},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.39},
  urldate = {2020-09-22},
  abstract = {Measuring the polarization of legislators and parties is a key step in understanding how politics develops over time. But in parliamentary systems---where ideological positions estimated from roll calls may not be informative---producing valid estimates is extremely challenging. We suggest a new measurement strategy, that makes innovative use of the `accuracy' of machine classifiers, i.e. the number of correct predictions made as a proportion of all predictions. In our case, the `labels' are the party identifications of the members of parliament, predicted from their speeches, along with some information on debate subjects. Intuitively, when the learner is able to discriminate members in the two main Westminster parties well, we claim we are in a period of `high' polarization. By contrast, when the classifier has low accuracy---and makes a relatively large number of mistakes in terms of allocating members to parties based on the data---we argue parliament is in an era of `low' polarization. This approach is fast and substantively valid, and we demonstrate its merits with simulations, and by comparing the estimates from 78 years of House of Commons speeches with qualitative and quantitative historical accounts of the same. As a headline finding, we note that contemporary British politics is approximately as polarized as it was in the mid1960s---that is, in the middle of the `post-war consensus'. More broadly, we show that the technical performance of supervised learning algorithms can be directly informative about substantive matters in social science.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\7ETPGGM5\Peterson et Spirling - 2018 - Classification Accuracy as a Substantive Quantity .pdf}
}

@article{quinn2010,
  title = {How to {{Analyze Political Attention}} with {{Minimal Assumptions}} and {{Costs}}},
  author = {Quinn, Kevin M. and Monroe, Burt L. and Colaresi, Michael and Crespin, Michael H. and Radev, Dragomir R.},
  year = {2010},
  month = jan,
  journal = {American Journal of Political Science},
  volume = {54},
  number = {1},
  pages = {209--228},
  issn = {00925853, 15405907},
  doi = {10.1111/j.1540-5907.2009.00427.x},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\MPJUFWFK\Quinn et al. - 2010 - How to Analyze Political Attention with Minimal As.pdf}
}

@article{ramey2019,
  title = {Measuring {{Elite Personality Using Speech}}},
  author = {Ramey, Adam J. and Klingler, Jonathan D. and Hollibaugh, Gary E.},
  year = {2019},
  month = jan,
  journal = {Political Science Research and Methods},
  volume = {7},
  number = {1},
  pages = {163--184},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2016.12},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\KD9TF66H\Ramey et al. - 2019 - Measuring Elite Personality Using Speech.pdf}
}

@article{rauh2022,
  title = {Supranational Emergency Politics? {{What}} Executives' Public Crisis Communication May Tell Us},
  shorttitle = {Supranational Emergency Politics?},
  author = {Rauh, Christian},
  year = {2022},
  month = jun,
  journal = {Journal of European Public Policy},
  volume = {29},
  number = {6},
  pages = {966--978},
  issn = {1350-1763, 1466-4429},
  doi = {10.1080/13501763.2021.1916058},
  urldate = {2023-04-30},
  abstract = {This contribution engages with the empirical analysis of emergency politics in the EU, arguing that executives' public communication helps to distinguish crisis management from crisis exploitation. An initial, descriptive text analysis of emergency emphasis in more than 19,000 executive speeches suggests that supranational actors, most notably the European Central Bank, do indeed use rather alarmist language over and beyond objective crisis pressures when their competences are contested. Yet, this behaviour does not appear to be a ubiquitous phenomenon, pointing to the need for more specific expectations on when and why EU executives pro-actively embark on the emergency politics script.},
  langid = {english},
  keywords = {measuring_ideology,NLP,political_science,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\PUKPP4MC\Rauh - 2022 - Supranational emergency politics What executives’.pdf}
}

@article{rheault2020,
  title = {Word {{Embeddings}} for the {{Analysis}} of {{Ideological Placement}} in {{Parliamentary Corpora}}},
  author = {Rheault, Ludovic and Cochrane, Christopher},
  year = {2020},
  month = jan,
  journal = {Political Analysis},
  volume = {28},
  number = {1},
  pages = {112--133},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2019.26},
  urldate = {2021-07-28},
  abstract = {Word embeddings, the coe icients from neural network models predicting the use of words in context, have now become inescapable in applications involving natural language processing. Despite a few studies in political science, the potential of this methodology for the analysis of political texts has yet to be fully uncovered. This paper introduces models of word embeddings augmented with political metadata and trained on large-scale parliamentary corpora from Britain, Canada, and the United States. We fit these models with indicator variables of the party a iliation of members of parliament, which we refer to as party embeddings. We illustrate how these embeddings can be used to produce scaling estimates of ideological placement and other quantities of interest for political research. To validate the methodology, we assess our results against indicators from the Comparative Manifestos Project, surveys of experts, and measures based on roll-call votes. Our findings suggest that party embeddings are successful at capturing latent concepts such as ideology, and the approach provides researchers with an integrated framework for studying political language.},
  langid = {english},
  keywords = {-Lu,Ideology,machine_learning,NLP,political_science,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\E2XH72VV\Rheault et Cochrane - 2020 - Word Embeddings for the Analysis of Ideological Pl.pdf}
}

@article{roberts2013a,
  title = {The {{Structural Topic Model}} and {{Applied Social Science}}},
  author = {Roberts, Margaret E and Tingley, Dustin and Stewart, Brandon M and Airoldi, Edoardo M},
  year = {2013},
  journal = {Advances in neural information processing systems workshop on topic models: computation, application, and evaluation},
  volume = {4},
  pages = {1--20},
  abstract = {We develop the Structural Topic Model which provides a general way to incorporate corpus structure or document metadata into the standard topic model. Document-level covariates enter the model through a simple generalized linear model framework in the prior distributions controlling either topical prevalence or topical content. We demonstrate the model's use in two applied problems: the analysis of open-ended responses in a survey experiment about immigration policy, and understanding differing media coverage of China's rise.},
  langid = {english},
  keywords = {-Lu,NLP,STM},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5BXTLUUA\Roberts et al. - 2013 - The Structural Topic Model and Applied Social Scie.pdf}
}

@article{roberts2014,
  title = {Structural {{Topic Models}} for {{Open-Ended Survey Responses}}: {{STRUCTURAL TOPIC MODELS FOR SURVEY RESPONSES}}},
  shorttitle = {Structural {{Topic Models}} for {{Open-Ended Survey Responses}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and {Leder-Luis}, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  year = {2014},
  month = oct,
  journal = {American Journal of Political Science},
  volume = {58},
  number = {4},
  pages = {1064--1082},
  issn = {00925853},
  doi = {10.1111/ajps.12103},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {-Lu,NLP,open-ended_survey,STM,Topic_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\4AWFSDEB\\GadarianAnalysis.R;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\ENCQDDY8\\ANESAnalysis(1).R;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\RFI8FRV6\\Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Resp.pdf}
}

@incollection{roberts2016,
  title = {Navigating the {{Local Modes}} of {{Big Data}}: {{The Case}} of {{Topic Models}}},
  shorttitle = {Navigating the {{Local Modes}} of {{Big Data}}},
  booktitle = {Computational {{Social Science}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin},
  editor = {Alvarez, R. Michael},
  year = {2016},
  pages = {51--97},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781316257340.004},
  urldate = {2020-09-22},
  isbn = {978-1-316-25734-0},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\69ETRTSC\Roberts et al. - 2016 - Navigating the Local Modes of Big Data The Case o.pdf}
}

@article{rodman2020,
  title = {A {{Timely Intervention}}: {{Tracking}} the {{Changing Meanings}} of {{Political Concepts}} with {{Word Vectors}}},
  shorttitle = {A {{Timely Intervention}}},
  author = {Rodman, Emma},
  year = {2020},
  month = jan,
  journal = {Political Analysis},
  volume = {28},
  number = {1},
  pages = {87--111},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2019.23},
  urldate = {2023-10-18},
  abstract = {Word vectorization is an emerging text-as-data method that shows great promise for automating the analysis of semantics---here, the cultural meanings of words---in large volumes of text. Yet successes with this method have largely been confined to massive corpora where the meanings of words are presumed to be fixed. In political science applications, however, many corpora are comparatively small and many interesting questions hinge on the recognition that meaning changes over time. Together, these two facts raise vexing methodological challenges. Can word vectors trace the changing cultural meanings of words in typical small corpora use cases? I test four time-sensitive implementations of word vectors (word2vec) against a gold standard developed from a modest data set of 161 years of newspaper coverage. I find that one implementation method clearly outperforms the others in matching human assessments of how public dialogues around equality in America have changed over time. In addition, I suggest best practices for using word2vec to study small corpora for time series questions, including bootstrap resampling of documents and pretraining of vectors. I close by showing that word2vec allows granular analysis of the changing meaning of words, an advance over other common text-as-data methods for semantic research questions.},
  langid = {english},
  keywords = {NLP,word_embeddings,word_meaning,word2vec},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\Y7T22T42\Rodman_2020_A Timely Intervention.pdf}
}

@article{rodriguez2023,
  title = {Embedding {{Regression}}: {{Models}} for {{Context-Specific Description}} and {{Inference}}},
  shorttitle = {Embedding {{Regression}}},
  author = {Rodriguez, Pedro L. and Spirling, Arthur and Stewart, Brandon M.},
  year = {2023},
  month = nov,
  journal = {American Political Science Review},
  volume = {117},
  number = {4},
  pages = {1255--1274},
  publisher = {{Cambridge University Press}},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055422001228},
  urldate = {2023-10-18},
  abstract = {Social scientists commonly seek to make statements about how word use varies over circumstances---including time, partisan identity, or some other document-level covariate. For example, researchers might wish to know how Republicans and Democrats diverge in their understanding of the term ``immigration.'' Building on the success of pretrained language models, we introduce the {\`a} la carte on text (conText) embedding regression model for this purpose. This fast and simple method produces valid vector representations of how words are used---and thus what words ``mean''---in different contexts. We show that it outperforms slower, more complicated alternatives and works well even with very few documents. The model also allows for hypothesis testing and statements about statistical significance. We demonstrate that it can be used for a broad range of important tasks, including understanding US polarization, historical legislative development, and sentiment detection. We provide open-source software for fitting the model.},
  langid = {english},
  keywords = {_tablet,NLP,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\Y2QLLMZU\Rodriguez et al_2023_Embedding Regression.pdf}
}

@article{roed2022,
  title = {When Do Political Parties Listen to Interest Groups?},
  author = {R{\o}ed, Maiken},
  year = {2022},
  month = jan,
  journal = {Party Politics},
  pages = {13540688211062832},
  publisher = {{SAGE Publications Ltd}},
  issn = {1354-0688},
  doi = {10.1177/13540688211062832},
  urldate = {2022-04-08},
  abstract = {This paper examines when parties listen to interest groups and adopt their input. Interest group information can help parties bolster their positions, and by taking their input into account, parties show that they are responsive to the groups' interests which can increase their appeal to their constituents. Listening to interest groups can, however, also repel voters who disagree with the groups' positions. This paper argues that party and issue-level characteristics affect whether the benefits of listening to interest groups exceed the costs. Examining more than 25,000 party-interest group observations on 88 Norwegian policy proposals and using a text reuse approach to measure interest group influence, the findings indicate that public salience, party issue emphasis, interest group coalitions, and government status affect parties' propensity to listen. This implies that interest groups can be a pertinent source of information for parties under certain circumstances which affects the link between voters and parties.},
  langid = {english},
  keywords = {interest_groups,language_similarity,NLP,Political_parties,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5RIJE8JJ\Røed - 2022 - When do political parties listen to interest group.pdf}
}

@article{sanders2017,
  title = {Themes and {{Topics}} in {{Parliamentary Oversight Hearings}}: {{A New Direction}} in {{Textual Data Analysis}}},
  shorttitle = {Themes and {{Topics}} in {{Parliamentary Oversight Hearings}}},
  author = {Sanders, James and Lisi, Giulio and {Schonhardt-Bailey}, Cheryl},
  year = {2017},
  month = dec,
  journal = {Statistics, Politics and Policy},
  volume = {8},
  number = {2},
  pages = {153--194},
  issn = {2151-7509, 2194-6299},
  doi = {10.1515/spp-2017-0012},
  urldate = {2021-07-28},
  abstract = {This paper contributes to the growing empirical work on deliberation in legislatures by proposing a novel approach to analysing parliamentary hearings using both thematic and topic modelling textual analysis software. We explore variations in deliberative quality across economic policy type (fiscal policy, monetary policy and financial stability) and across parliamentary chambers (Commons and Lords) in UK select committee oversight hearings during the 2010-2015 Parliament. Our overall focus is not only to suggest a multi-method approach to the textual analysis of parliamentary data, but also to explore more substantive aspects of parliamentary oversight, such as: (1) the extent to which oversight varies between unelected and elected policy makers; and (2) whether parliamentarians conduct oversight more forcefully or more along partisan lines when they are challenging fellow politicians as opposed to central bank officials. Our findings suggest consistent differences in deliberative styles between types of hearings (fiscal, monetary, financial stability) and between chambers (Commons, Lords).},
  langid = {english},
  keywords = {Hearings,NLP,political_science,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\NUG8L2JY\Sanders et al. - 2017 - Themes and Topics in Parliamentary Oversight Heari.pdf}
}

@article{schwartzberg2023,
  title = {Equals, {{Peers}} and {{Free-born Englishmen}}},
  author = {Schwartzberg, Melissa and Spirling, Arthur},
  year = {2023},
  abstract = {We consider the evolution of Leveller thought relative to their contemporaries during the English Civil War(s). We compile a new data set of hundreds of seventeenth century pamphlets to chart themes related to rights, elections and suffrage for a variety of actors for the period 1638--1666. We combine this with novel word embedding techniques trained on millions of Early Modern English documents to make statements about how actors of the time understood key issues of interest to modern democratic theory. Our quantitative and qualitative results show that inter alia (1) John Lilburne created the key rhetoric of ``freeborn Englishman'' but that its evolution is more complex than assumed; (2) the Leveller interest in ``equality'' is distinctively legal rather than ``social'' in nature; (3) this legal conception also extends to Leveller concerns with elections, but on this topic they become more aligned with their contemporaries over time than in the case of equality.},
  langid = {english},
  keywords = {_tablet,NLP,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\PD43MUEL\Schwartzberg_Spirling_2023_Equals, Peers and Free-born Englishmen.pdf}
}

@article{slapin2008,
  title = {A {{Scaling Model}} for {{Estimating Time-Series Party Positions}} from {{Texts}}},
  author = {Slapin, Jonathan B. and Proksch, Sven-Oliver},
  year = {2008},
  month = jul,
  journal = {American Journal of Political Science},
  volume = {52},
  number = {3},
  pages = {705--722},
  issn = {00925853, 15405907},
  doi = {10.1111/j.1540-5907.2008.00338.x},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {Ideology,NLP,political_science,wordfish_model},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5UJBJCW3\Slapin et Proksch - 2008 - A Scaling Model for Estimating Time-Series Party P.pdf}
}

@inproceedings{soper2012,
  title = {Who {{Are We}}? {{Mining Institutional Identities Using}} n-Grams},
  shorttitle = {Who {{Are We}}?},
  booktitle = {2012 45th {{Hawaii International Conference}} on {{System Sciences}}},
  author = {Soper, Daniel S. and Turel, Ofir},
  year = {2012},
  month = jan,
  pages = {1107--1116},
  issn = {1530-1605},
  doi = {10.1109/HICSS.2012.642},
  abstract = {Disciplines and organizations alike can be defined by the text they produce, the topics they discuss, and the language they employ. Analyzing such large amounts of text is challenging, but is nevertheless needed because it can help stakeholders to understand key themes in, and the evolution of their corporate or disciplinary identity. N-gram analysis is a leading text-mining technique that can be leveraged for this purpose. In this manuscript we present the development and demonstrate the potential utility of an n-gram analysis tool. We focus on revealing several aspects of the identity of an academic journal, namely Communications of the ACM (CACM), through the analysis of over 14 million unique n-grams and their relative frequencies. The results of the study imply that n-gram analyses may be a key tool in resolving the IS identity crisis. Implications for research and practice are discussed.},
  keywords = {NLP,Quantitative_Analysis},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\II2NLS96\Soper et Turel - 2012 - Who Are We Mining Institutional Identities Using .pdf}
}

@article{spirling2012,
  title = {U.{{S}}. {{Treaty Making}} with {{American Indians}}: {{Institutional Change}} and {{Relative Power}}, 1784-1911: {{TREATY MAKING WITH AMERICAN INDIANS}}},
  shorttitle = {U.{{S}}. {{Treaty Making}} with {{American Indians}}},
  author = {Spirling, Arthur},
  year = {2012},
  month = jan,
  journal = {American Journal of Political Science},
  volume = {56},
  number = {1},
  pages = {84--97},
  issn = {00925853},
  doi = {10.1111/j.1540-5907.2011.00558.x},
  urldate = {2020-09-22},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\JT9WHET8\Spirling - 2012 - U.S. Treaty Making with American Indians Institut.pdf}
}

@article{stoltz2019,
  title = {Concept {{Mover}}'s {{Distance}}: Measuring Concept Engagement via Word Embeddings in Texts},
  shorttitle = {Concept {{Mover}}'s {{Distance}}},
  author = {Stoltz, Dustin S. and Taylor, Marshall A.},
  year = {2019},
  month = jul,
  journal = {Journal of Computational Social Science},
  volume = {2},
  number = {2},
  pages = {293--313},
  issn = {2432-2725},
  doi = {10.1007/s42001-019-00048-6},
  urldate = {2023-04-30},
  abstract = {We propose a method for measuring a text's engagement with a focal concept using distributional representations of the meaning of words. More specifically, this measure relies on word mover's distance, which uses word embeddings to determine similarities between two documents. In our approach, which we call Concept Mover's Distance, a document is measured by the minimum distance the words in the document need to travel to arrive at the position of a ``pseudo document'' consisting of only words denoting a focal concept. This approach captures the prototypical structure of concepts, is fairly robust to pruning sparse terms as well as variation in text lengths within a corpus, and with pre-trained embeddings, can be used even when terms denoting concepts are absent from corpora and can be applied to bag-of-words datasets. We close by outlining some limitations of the proposed method as well as opportunities for future research.},
  langid = {english},
  keywords = {measuring_ideology,NLP,political_science,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\5LW2U3GK\Stoltz_Taylor_2019_Concept Mover’s Distance.pdf}
}

@article{stoltz2019a,
  title = {Concept {{Mover}}'s {{Distance}}: Measuring Concept Engagement via Word Embeddings in Texts},
  shorttitle = {Concept {{Mover}}'s {{Distance}}},
  author = {Stoltz, Dustin S. and Taylor, Marshall A.},
  year = {2019},
  month = jul,
  journal = {Journal of Computational Social Science},
  volume = {2},
  number = {2},
  pages = {293--313},
  issn = {2432-2725},
  doi = {10.1007/s42001-019-00048-6},
  urldate = {2024-01-03},
  abstract = {We propose a method for measuring a text's engagement with a focal concept using distributional representations of the meaning of words. More specifically, this measure relies on word mover's distance, which uses word embeddings to determine similarities between two documents. In our approach, which we call Concept Mover's Distance, a document is measured by the minimum distance the words in the document need to travel to arrive at the position of a ``pseudo document'' consisting of only words denoting a focal concept. This approach captures the prototypical structure of concepts, is fairly robust to pruning sparse terms as well as variation in text lengths within a corpus, and with pre-trained embeddings, can be used even when terms denoting concepts are absent from corpora and can be applied to bag-of-words datasets. We close by outlining some limitations of the proposed method as well as opportunities for future research.},
  langid = {english},
  keywords = {concept_mover_distance,cultural_sociology,NLP,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\QKW2L6BK\Stoltz_Taylor_2019_Concept Mover’s Distance.pdf}
}

@article{stoltz2021,
  title = {Cultural Cartography with Word Embeddings},
  author = {Stoltz, Dustin S. and Taylor, Marshall A.},
  year = {2021},
  month = oct,
  journal = {Poetics},
  series = {Measure {{Mohr Culture}}},
  volume = {88},
  pages = {101567},
  issn = {0304-422X},
  doi = {10.1016/j.poetic.2021.101567},
  urldate = {2024-01-03},
  abstract = {Using the frequency of keywords is a classic approach in the formal analysis of text, but has the drawback of glossing over the relationality of word meanings. Word embedding models overcome this problem by constructing a standardized and continuous ``meaning space'' where words are assigned a location based on relations of similarity to other words depending on how they are used in natural language samples. We show how word embeddings are commensurate with prevailing theories of meaning in sociology and can be put to the task of interpretation via two kinds of navigation. First, one can hold terms constant and measure how the embedding space moves around them---much like astronomers measured the changing of celestial bodies with the seasons. Second, one can also hold the embedding space constant and see how documents or authors move relative to it---just as ships use the stars on a given night to determine their location. Using the empirical case of immigration discourse in the United States, we demonstrate the merits of these two broad strategies for advancing important topics in cultural theory, including social marking, media fields, echo chambers, and cultural diffusion and change more broadly.},
  keywords = {cultural_sociology,NLP,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\8USKKNQX\Stoltz_Taylor_2021_Cultural cartography with word embeddings.pdf}
}

@article{taddy2013,
  title = {Multinomial {{Inverse Regression}} for {{Text Analysis}}},
  author = {Taddy, Matt},
  year = {2013},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {108},
  number = {503},
  pages = {755--770},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2012.734168},
  urldate = {2020-09-22},
  abstract = {Text data, including speeches, stories, and other document forms, are often connected to sentiment variables that are of interest for research in marketing, economics, and elsewhere. It is also very high dimensional and difficult to incorporate into statistical analyses. This article introduces a straightforward framework of sentiment-sufficient dimension reduction for text data. Multinomial inverse regression is introduced as a general tool for simplifying predictor sets that can be represented as draws from a multinomial distribution, and we show that logistic regression of phrase counts onto document annotations can be used to obtain low dimension document representations that are rich in sentiment information. To facilitate this modeling, a novel estimation technique is developed for multinomial logistic regression with very highdimension response. In particular, independent Laplace priors with unknown variance are assigned to each regression coefficient, and we detail an efficient routine for maximization of the joint posterior over coefficients and their prior scale. This `gamma-lasso' scheme yields stable and effective estimation for general high-dimension logistic regression, and we argue that it will be superior to current methods in many settings. Guidelines for prior specification are provided, algorithm convergence is detailed, and estimator properties are outlined from the perspective of the literature on non-concave likelihood penalization. Related work on sentiment analysis from statistics, econometrics, and machine learning is surveyed and connected. Finally, the methods are applied in two detailed examples and we provide out-of-sample prediction studies to illustrate their effectiveness.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\MQKJS42R\Taddy - 2013 - Multinomial Inverse Regression for Text Analysis.pdf}
}

@article{taylor2020,
  title = {Concept {{Class Analysis}}: {{A Method}} for {{Identifying Cultural Schemas}} in {{Texts}}},
  shorttitle = {Concept {{Class Analysis}}},
  author = {Taylor, Marshall A. and Stoltz, Dustin S.},
  year = {2020},
  month = nov,
  journal = {Sociological Science},
  volume = {7},
  pages = {544--569},
  issn = {2330-6696},
  doi = {10.15195/v7.a23},
  urldate = {2024-01-03},
  abstract = {Recent methodological work at the intersection of culture, cognition, and computational methods has drawn attention to how cultural schemas can be},
  langid = {american},
  keywords = {cultural_sociology,NLP,word_embeddings},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\AEZNK44Z\Taylor_Stoltz_2020_Concept Class Analysis.pdf}
}

@article{tortola2019,
  title = {The Charismatic Leadership of the {{ECB}} Presidency: {{A}} Language-Based Analysis},
  shorttitle = {The Charismatic Leadership of the {{ECB}} Presidency},
  author = {Tortola, Pier Domenico and Pansardi, Pamela},
  year = {2019},
  journal = {European Journal of Political Research},
  volume = {58},
  number = {1},
  pages = {96--116},
  publisher = {{Wiley Online Library}},
  keywords = {Central_Bank_Communication,ECB,European_Studies,NLP,political_leadership,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\SRV53FZK\Tortola et Pansardi - 2019 - The charismatic leadership of the ECB presidency .pdf}
}

@article{tortola2020,
  title = {The {{Politicization}} of the {{European Central Bank}}: {{What Is It}}, and {{How}} to {{Study It}}?},
  shorttitle = {The {{Politicization}} of the {{European Central Bank}}},
  author = {Tortola, Pier Domenico},
  year = {2020},
  journal = {JCMS: Journal of Common Market Studies},
  volume = {58},
  number = {3},
  pages = {501--513},
  publisher = {{Wiley Online Library}},
  keywords = {ECB,European_Studies,Network_Analysis,NLP,political_science,Politicisation/depoliticisation},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\LPFTQECG\Tortola - 2020 - The Politicization of the European Central Bank W.pdf}
}

@article{truffa2022,
  title = {Undergraduate Gender Diversity and Direction of Scientific Research},
  author = {Truffa, Francesca and Wong, Ashley},
  year = {2022},
  keywords = {gender_biases,NLP,scientometrics},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\EN8MU96E\Truffa_Wong_2022_Undergraduate gender diversity and direction of scientific research.pdf}
}

@article{turney2010,
  title = {From {{Frequency}} to {{Meaning}}: {{Vector Space Models}} of {{Semantics}}},
  shorttitle = {From {{Frequency}} to {{Meaning}}},
  author = {Turney, P. D. and Pantel, P.},
  year = {2010},
  month = feb,
  journal = {Journal of Artificial Intelligence Research},
  volume = {37},
  pages = {141--188},
  issn = {1076-9757},
  doi = {10.1613/jair.2934},
  urldate = {2020-09-22},
  abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term--document, word--context, and pair--pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
  langid = {english},
  keywords = {NLP},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\P2PNKSS5\Turney et Pantel - 2010 - From Frequency to Meaning Vector Space Models of .pdf}
}

@article{tvinnereim2015,
  title = {Explaining Topic Prevalence in Answers to Open-Ended Survey Questions about Climate Change},
  author = {Tvinnereim, Endre and Fl{\o}ttum, Kjersti},
  year = {2015},
  journal = {Nature Climate Change},
  volume = {5},
  number = {8},
  pages = {744--747},
  publisher = {{Nature Publishing Group}},
  keywords = {Climate_change,NLP,open-ended_survey,Topic_modeling},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\JZ9K6R53\Tvinnereim et Fløttum - 2015 - Explaining topic prevalence in answers to open-end.pdf}
}

@article{vesan2021,
  title = {Speaking Social {{Europe}}: {{A}} Paradigmatic Shift in the {{European Commission Presidents}}' Social Policy Discourse?},
  shorttitle = {Speaking Social {{Europe}}},
  author = {Vesan, Patrik and Pansardi, Pamela},
  year = {2021},
  journal = {Journal of European Social Policy},
  pages = {0958928721999596},
  publisher = {{SAGE Publications Sage UK: London, England}},
  keywords = {European_Commission,NLP,political_science},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\GF856TH2\Vesan et Pansardi - 2021 - Speaking social Europe A paradigmatic shift in th.pdf}
}

@techreport{wehrheim2021,
  type = {{{workingPaper}}},
  title = {The {{Sound}} of {{Silence}}. {{On}} the ({{In}})Visibility of {{Economists}} in the {{Media}}},
  author = {Wehrheim, Lino},
  year = {2021},
  month = apr,
  institution = {{Humboldt-Universit{\"a}t zu Berlin}},
  issn = {2510-053X},
  doi = {10.18452/22794},
  urldate = {2021-05-12},
  abstract = {One way for economists to influence economic policy and society as a whole is to shape what Robert Shiller has called ``economic narratives''. This, in turn, puts the media in their role as professional storytellers in a central position. In this paper, I investigate how economists have been covered by the media in a long-term perspective. Particularly, I address two questions: How has the quantitative visibility of economists in the media developed over time? And how can news stories covering economists be characterized in terms of their content? I answer these questions in two steps. First, I provide a comparison of economists' quantitative media visibility in international newspapers. Second, building on a corpus of more than 12,000 newspaper articles, I conduct a case study on the German Council of Economic Experts. Using various text mining approaches, I survey four features of newspaper coverage:  topics, tonality, temporal perspective, and the role of individuals. Finally, based on extensive close reading, I briefly discuss two key turning points in the media history of economists, namely the 1980s and the late 1990s/early 2000s. The main finding is that economists have indeed become silent compared to their heyday of economic expertise in the 1960s, but that they have not been as silent as is often claimed.},
  copyright = {http://rightsstatements.org/vocab/InC/1.0/},
  langid = {english},
  keywords = {-Lu,Economics_and_media,expertise,NLP,Quantitative_Analysis},
  annotation = {Accepted: 2021-04-23T11:21:43Z},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\7ARFI2LW\Wehrheim - 2021 - The Sound of Silence. On the (In)visibility of Eco.pdf}
}

@article{welbers2017,
  title = {Text {{Analysis}} in {{R}}},
  author = {Welbers, Kasper and Van Atteveldt, Wouter and Benoit, Kenneth},
  year = {2017},
  month = oct,
  journal = {Communication Methods and Measures},
  volume = {11},
  number = {4},
  pages = {245--265},
  issn = {1931-2458, 1931-2466},
  doi = {10.1080/19312458.2017.1387238},
  urldate = {2020-09-22},
  abstract = {Computational text analysis has become an exciting research field with many applications in communication research. It can be a difficult method to apply, however, because it requires knowledge of various techniques, and the software required to perform most of these techniques is not readily available in common statistical software packages. In this teacher's corner, we address these barriers by providing an overview of general steps and operations in a computational text analysis project, and demonstrate how each step can be performed using the R statistical software. As a popular open-source platform, R has an extensive user community that develops and maintains a wide range of text analysis packages. We show that these packages make it easy to perform advanced text analytics.},
  langid = {english},
  keywords = {-Lu,NLP,Topic_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\8HMH7GI2\\Welbers et al. - 2017 - Text Analysis in R.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\UPF3PR6N\\Welbers et al. - 2017 - Text Analysis in R.pdf}
}

@article{wilkerson2017,
  title = {Large-{{Scale Computerized Text Analysis}} in {{Political Science}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Large-{{Scale Computerized Text Analysis}} in {{Political Science}}},
  author = {Wilkerson, John and Casas, Andreu},
  year = {2017},
  month = may,
  journal = {Annual Review of Political Science},
  volume = {20},
  number = {1},
  pages = {529--544},
  issn = {1094-2939, 1545-1577},
  doi = {10.1146/annurev-polisci-052615-025542},
  urldate = {2021-07-31},
  abstract = {Text has always been an important data source in political science. What has changed in recent years is the feasibility of investigating large amounts of text quantitatively. The internet provides political scientists with more data than their mentors could have imagined, and the research community is providing accessible text analysis software packages, along with training and support. As a result, text-as-data research is becoming mainstream in political science. Scholars are tapping new data sources, they are employing more diverse methods, and they are becoming critical consumers of findings based on those methods. In this article, we first describe the four stages of a typical text-as-data project. We then review recent political science applications and explore one important methodological challenge---topic model instability---in greater detail.},
  langid = {english},
  keywords = {machine_learning,NLP,political_science,quality_metrics,survey,Topic_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\4RRP3ZWQ\\Wilkerson et Casas - 2017 - Large-Scale Computerized Text Analysis in Politica.pdf;C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\5MRKZ6S4\\Wilkerson et Casas - 2017 - Large-Scale Computerized Text Analysis in Politica.pdf}
}

@article{young2012,
  title = {Affective {{News}}: {{The Automated Coding}} of {{Sentiment}} in {{Political Texts}}},
  shorttitle = {Affective {{News}}},
  author = {Young, Lori and Soroka, Stuart},
  year = {2012},
  month = apr,
  journal = {Political Communication},
  volume = {29},
  number = {2},
  pages = {205--231},
  issn = {1058-4609, 1091-7675},
  doi = {10.1080/10584609.2012.671234},
  urldate = {2020-09-22},
  abstract = {An increasing number of studies in political communication focus on the ``sentiment'' or ``tone'' of news content, political speeches, or advertisements. This growing interest in measuring sentiment coincides with a dramatic increase in the volume of digitized information. Computer automation has a great deal of potential in this new media environment. The objective here is to outline and validate a new automated measurement instrument for sentiment analysis in political texts. Our instrument uses a dictionary-based approach consisting of a simple word count of the frequency of keywords in a text from a predefined dictionary. The design of the freely available Lexicoder Sentiment Dictionary (LSD) is discussed in detail here. The dictionary is tested against a body of human-coded news content, and the resulting codes are also compared to results from nine existing content-analytic dictionaries. Analyses suggest that the LSD produces results that are more systematically related to human coding than are results based on the other available dictionaries. The LSD is thus a useful starting point for a revived discussion about dictionary construction and validation in sentiment analysis for political communication.},
  langid = {english},
  keywords = {dictionary_methods,NLP,sentiment_analysis},
  file = {C:\Users\goutsmedt\Documents\MEGAsync\Zotero\storage\MJ8Q5QHF\Young et Soroka - 2012 - Affective News The Automated Coding of Sentiment .pdf}
}
