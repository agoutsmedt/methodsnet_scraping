
---
title: "Mastering Web Scraping for Data Collection"
subtitle: "MethodsNET Workshop"
author: "Aurélien Goutsmedt, Thomas Laloux and Marine Bardou (UCLouvain)"
date: "11/01/2024"
format:
  revealjs: 
    theme: 
      [solarized, custom.scss]
    transition: convex
    slide-number: true
    toc: false 
    toc-depth: 1
    slide-level: 2
    number-sections: true
    number-depth: 1
    css: "custom.css"
    embed-resources: true
    view-distance: 100
    progress: true
    width: 1200
    margin: 0.01
    preview-links: true
    code-tools: true
    code-line-numbers: false
    code-copy: true
    code-link: true
    code-overflow: wrap
    code-block-height: 400px
lightbox: auto
bibliography: "bibliography.bib" 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, echo = TRUE)
```

# Training Goals

## Motivations {.smaller}

:::{.incremental .highlight-last}

### First Session:

- Giving a basic understanding of what web scraping is and what it can do
- Discussing ethical (and legal) issues linked to web scraping
- Proposing a roadmap for understanding how to proceed for practising web scraping using R
- Providing bits of codes and practical tips

<br> 

### Second Session:

- Hands-on practice with different exercises by level of difficulty

:::

::: {.notes}
**Slide presenter**: Aurélien
- Second session: even if no particular knowledge of R, you can try your hand on it and we'll be able to guide you.
 - Time for asking participants skills in R
:::

## Download the documents

[https://github.com/agoutsmedt/methodsnet_scraping/](https://github.com/agoutsmedt/methodsnet_scraping/)

## Requisits

:::{.incremental .highlight-last}
- Need of R and RStudio for the second session (please make sure to install them!)
- These slides are built from a ```.qmd``` (quarto) document $\Rightarrow$ all the codes used in these slides can be run in RStudio
:::

::: {.fragment .small-text}
```{r installing-packages}
# These lines of code has to be run before if you want to install all the packages directly

# pacman will be used to install (if necessary) and load packages
if(length(grep("pacman", installed.packages())) == 0) install.packages("pacman") # check if installed
library(pacman)

# Installing the needed packages in advance
p_load(tidyverse, # basic suite of packages
       glue, # useful for building string (notably for url)
       scico, # color palettes
       patchwork, # for juxtaposition of graphs
       DT) # to display html tables
```
:::

```{r}
#| echo: FALSE
p_load(here)
```

::: {.notes}
**slide presenter**: Aurélien
:::

# What is web Scraping

## What is web scraping ?

:::{.incremental .highlight-last}
:::{.small-text}
- The **web scraping** is a method for automatically extracting data available in the World Wide Web 
- The World Wide Web, or "Web", is a network of websites (online documents coded in ```html``` and ```css```)
- A **web scraper** is a program, for instance in  ```R ```, that automatically read the ```html``` structure of a website and extract the relevant content (text, hypertext references, tables)
  - No need to fully understand ```html``` and ```css```
- Useful when many pages to scrape

:::
:::

::: {.notes}
**slide presenter**: Aurélien
:::

## What is HTML and CSS?

:::{.columns}
:::{.column width="50%"}

![](pictures/exemple_wikipedia.png){width=91% fig-align="center"}

:::
:::{.column width="50%"}
:::{.fragment}

![](pictures/exemple_html.png){width=88% fig-align="center"}

:::
:::
:::

::: {.notes}
**slide presenter**: Aurélien
:::

## API vs. web scraping


:::{.incremental .highlight-last .small-text}
:::{.fragment .nonincremental}
- **API (Application Programming Interface)** provides a structured and predictable way to retrieve data from a service. It's like ordering from a menu; you request specific data and receive it in a structured format
  - [Twitter API](https://developer.x.com/en/docs/x-api)
  - [Wikidata API](https://www.wikidata.org/wiki/Wikidata:REST_API)
  - API for bibliometric data like Scopus, Web of Science, Google Scholar...

:::
- Web Scraping is the process of programmatically extracting data from the web page's HTML itself. It's akin to manually copying information from a book; you decide what information you need and how to extract it

:::

::: {.notes}
**slide presenter**: Aurélien
:::

## API vs. web scraping

:::{.incremental .highlight-last .small-text}
- **Control and Structure**: APIs offer structured access to data, whereas web scraping requires parsing HTML and often cleaning the data yourself.
- **Ease of Use**: Using an API can be simpler since it's designed for data access (but not always the case). Scraping requires dealing with HTML changes and is more prone to breaking.
- **Availability**: Not all websites offer an API, making web scraping a necessity in some cases.
- **Limitations and Authorization**: APIs often have rate limits and may require authentication, but approve access to the data. Web scraping can bypass these limits but might violate terms of service.
:::

::: {.notes}
**slide presenter**: Aurélien
:::

## Forget about big data, small data is everywhere!

:::{.incremental .highlight-last}
- A large possibility of data you can collect:
  - official documents/speeches
  - agenda and meetings
  - list of personnel or experts in commission
  - laws or negotiations
- To take into account the development of pages over time, we can do it through the [Internet Archive](http://web.archive.org/)
:::

::: {.notes}
**slide presenter**: Thomas

- Examples of project:
  - Creating a database of central banks documents
  - Database of the meetings of commissioners
  - Prosopographic database: scraping public profiles, notably via LinkedIn.
  - Other examples on our own research?

Do you have ideas of data you may be interested to collect? Why you did not do it?
:::

## Building databases {.smaller}

### Involves a series of questions:

:::{.incremental .highlight-last}
 
- What's your research question and which data would be appropriate to answer it?
- How much data to collect?
  - Trade-off between collecting a lot of information (which requires more time) and risking to miss some information at a later step
- How to scrape the data? In which format?
  - Interaction between extracting data properly in a first step or cleaning it in a second step
- What do you loose by doing it automatically rather than manually? (or the reverse)
- How to analyse/understand my new data?
- How to update my database?

:::

::: {.notes}
**Slide presenter**
Thomas
:::

# The Ethics of Web Scraping

## Ethical considerations

:::{.incremental .highlight-last  .small-text}
- **Legal Considerations**: Not all data is free to scrape. Websites' terms of service may explicitly forbid web scraping, and in some jurisdictions, scraping can have legal implications
  - What is "forbidden" by a website is not necessary "illegal"
- **Privacy Concerns**: Scraping personal data can raise significant privacy issues and may be subject to regulations like GDPR in Europe
- **Website Performance**: Scraping, especially if aggressive (e.g., making too many requests in a short period), can negatively impact the performance of a website, affecting its usability for others
:::

::: {.notes}
**Slide presenter**
Thomas
General Data Protection Regulation
If you look closely, you’ll find many websites include a “terms and conditions” or “terms of service” link somewhere on the page, and if you read that page closely you’ll often discover that the site specifically prohibits web scraping. These pages tend to be a legal land grab where companies make very broad claims. It’s polite to respect these terms of service where possible, but take any claims with a grain of salt.
US courts have generally found that simply putting the terms of service in the footer of the website isn’t sufficient for you to be bound by them

Exemple OKcupid

While the researchers felt that there was nothing wrong with this since the data were already public, this work was widely condemned due to ethics concerns around identifiability of users whose information was released in the dataset. If your work involves scraping personally identifiable information, we strongly recommend reading about the OkCupid study3 as well as similar studies with questionable research ethics involving the acquisition and release of personally identifiable information.

General Data Protection Regulation
:::

## Questions at stake [@krotov2020]

![@krotov2020](pictures/legality_scraping.png){width=86% fig-align="center"}

:::{.notes}
- No legislation specific to web scraping exist.
  - May be impossible to determine the legality of some scraping actions.
- Uncertainty about violating the "terms of use" or "terms of service" is a "fraudulent" use of data. Often, should be motivated and not consider illegal as such. It has been also noted for instance that "Web Scraping publicly available data likely does not violate the CFAA". (CFAA = Computer Fraud and Abuse Act)
- Breach of contract: should have incurred material damages.
- Trespass to chattels: when have incurred material damages.
- Trade Secrets: when used for deliberate surveillance
:::

::: {.notes}
**Slide presenter**
Thomas
:::

## Ethical practices

:::{.incremental .highlight-last}
- **Respect ```robots.txt```**: This file on websites indicates which parts should not be scraped
- **Rate Limiting**: Making requests at a reasonable rate to avoid overloading the website's server
- **User-Agent String**: Identifying your scraper can help website owners understand the nature of the traffic
- **Data Use**: Consider the ethical implications of how scraped data is used. Ensure it respects the privacy and rights of individuals

:::

::: {.notes}
**Slide presenter**
Thomas
 In any case, you should be respectful of the resources of the server hosting the pages you are scraping. Most importantly, this means that if you’re scraping many pages, you should make sure to wait a little between each request.
:::

## Ethical practices

:::{.incremental .highlight-last}

- Can my actions in relation to Web data produce harm to individuals, organizations, or
communities?

- What can I do to provide reasonable assurance that this unintended harm does not happen?

:::{.fragment}
> **If the data is public, non-personal, and factual, you're likely to be ok**
:::
:::

::: {.notes}
- **Slide presenter** : Thomas

If the data isn’t public, non-personal, or factual or you’re scraping the data specifically to make money with it, you’ll need to talk to a lawyer.
:::

# How to scrape a website? 

## What is R? 

:::{.incremental .highlight-last}
- Programming language created at the end of 1990s for statistical computing 
- Free and open source
- Enriched by a large set of packages
- Generally used in the development environment RStudio 

:::

::: {.notes}
**Slide presenter**
Marine
:::

## R basic concepts 

:::{.incremental .highlight-last .small-text}
- **object**: like a box to which you assign a name and put various things in
- **data frame** (it is a specific type of object!): comparable to a spreadsheet

:::{.fragment}
```{r}
#| output-location: fragment
firstnames <- c("Anna", "Laura", "Lise")
firstnames
```
:::
:::{.fragment}
```{r}
#| output-location: fragment
ages <- c(26,28,25)
ages
```
:::
:::{.fragment}
```{r}
#| output-location: fragment
discipline <- c("maths", "sociology", "law")
discipline
```
:::
:::{.fragment}
```{r}
#| output-location: fragment
my_data <- data.frame(firstnames, ages, discipline)
my_data
```
:::
:::

::: {.notes}
**Slide presenter**
Marine
:::

## R basic concepts 

:::{.incremental .highlight-last}
- **function**: a block of code to which you provide input and which returns an output. You can define yourself a function, but many are pre built. 

:::{.fragment}
```{r}
#| output-location: fragment
length(firstnames)
```
:::

- **package**: most importantly, packages contain functions! 

:::{.fragment}
```{r}
#| output-location: fragment
library(stringr)
str_detect(firstnames, "Anna")
```
:::
:::

::: {.notes}
**Slide presenter**
Marine
:::

##  Useful packages for webscraping in R

:::{.incremental .highlight-last}
- [rvest](https://rvest.tidyverse.org/): navigating website, scraping and cleaning ```html``` code
- [polite](https://dmi3kno.github.io/polite/): responsible web etiquette (informing the website that you are scraping)
- [RSelenium](https://docs.ropensci.org/RSelenium/index.html): using a bot to interact with a website

:::{.fragment}
```{r}
p_load(rvest, # scraping and manipulating html pages
       polite, # scraping ethically
       RSelenium) # scraping by interacting with websites
```

- Additional tool to navigate ```html``` and ```css``` code: [SelectorGadget](https://selectorgadget.com/)  or [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/)
:::
:::

::: {.notes}
**Slide presenter**
Marine
:::

## Navigating the website

:::{.incremental .highlight-last .small-text}

::: {.fragment}
```{r}
bis_website_path <- "https://www.bis.org/cbspeeches/index.htm"
```
:::

::: {.fragment .nonincremental}
- Two basic ways to scrape a series of pages:
  - Interacting with the website by using a bot browser (the more complicated one, using ```RSelenium```)
  - Understanding/extracting the "structure" of the website (only need ```rvest``` in many cases)
:::

::: {.fragment .nonincremental}
- Cases in which you need to interact with the website
  - When you need to scroll to have the full page content
  - When you need to open menus to display all the page content
  - When you need to type in a search bar and run a search
  
:::
:::

::: {.notes}

**Slide presenter**: Marine

Page BIS: we will extract date, titles, etc... But we don't want just the first page, so before to do that, we need to understand how to navigate between pages. 
:::

## The Role of Sitemaps

:::{.incremental .highlight-last .small-text}
- Sitemap: to inform search engines about URLs on a website that are available for web crawling
  - Understand the structure of a website
  - Find where is the information we want to extract

:::{.fragment .nonincremental}
- In general : 
  - `<domain>/sitemap.xml`
  - `<domain>/sitemap_index.xml`
  - `<domain>/sitemap`
  - for BIS : [https://www.bis.org/sitemap.xml](https://www.bis.org/sitemap.xml)

:::
:::

::: {.notes}
**slide presenter**: Aurélien
:::

## Being respectful of the website {.smaller .scrollable}

::: {.fragment}

Declaring yourself:
```{r eval=TRUE}
session <- polite::bow(bis_website_path, 
                       user_agent = "polite R package - used for academic training by 
                       Aurélien Goutsmedt (aurelien.goutsmedt[at]uclouvain.be)")
```
:::

::: {.fragment}
```{r eval=TRUE}
#| output-location: fragment
cat(session$robotstxt$text)
```
:::

::: {.fragment}
```{r eval=TRUE}
#| output-location: fragment
session$robotstxt$sitemap
```
:::


::: {.notes}
**slide presenter**: Aurélien

Visit the Sitemap
:::

## Using sitemap {.scrollable .smaller}


```{r eval=TRUE}
#| code-fold: true

# This function goes to a sitemap page, and extract all the urls found
extract_url_from_sitemap <- function(url, delay = 1) { 
  urls <- read_html(url) %>% 
    html_elements(xpath = ".//loc") %>% 
    html_text()
  Sys.sleep(delay) # You set a delay to avoid overloading the website
  return(urls)
}

# insistently allows to retry when you did not succeed in loading the page
insistently_extract_url <- insistently(extract_url_from_sitemap, 
                                       rate = rate_backoff(max_times = 5)) 

document_pages <- extract_url_from_sitemap(session$robotstxt$sitemap$value) %>% 
  .[str_detect(., "documents")] # We keep only the URLs for documents

bis_pages <- map(document_pages[1:5], # showing the code just on the first five years
                 ~insistently_extract_url(url = ., 
                                          delay = session$delay))

bis_pages <- tibble(year = str_extract(document_pages[1:5], "\\d{4}"),
                    urls = bis_pages) %>% 
  unnest(urls)
```


```{r eval=TRUE}
#| echo: FALSE
datatable(bis_pages,
          rownames = FALSE,
          class = 'cell-border stripe',
          options = list(
            searching = FALSE, # This removes the search bar
            pageLength = 10, # This sets the initial number of rows displayed to 5
            dom = "tip")) %>%
  # Add custom style to reduce font size
  htmlwidgets::onRender("
    function(el, x) {
      $(el).css('font-size', '14px');  // Adjust the font size here
      $(el).find('th').css('font-size', '12px');  // Adjust the header font size
    }
  ")
```


::: {.notes}
**slide presenter**: Aurélien
:::

## The key steps of web scraping 

There are many ways to scrape, but your baseline scenario is:

:::{.incremental .highlight-last}
1. **Read the html code** of the page 
(you can access this page with an url directly, or after interacting with it using RSelenium). 

2. **Extract** some specific elements in that page using selectors. 

3. **Read the text** (or some other info) behind contained in these elements. 

4. **Store** the information retrieved in a data frame. 
:::

::: {.notes}
**slide presenter**: Marine
:::

## Scraping a BIS speech with rvest 

![[https://www.bis.org/review/r241022f.htm](https://www.bis.org/review/r241022f.htm)](pictures/text_BIS.png){width=86% fig-align="center"}


::: {.notes}
**slide presenter**: Marine
:::

## Scraping one page: using a scrape helper

:::{.incremental .highlight-last}
- Scraping add-ons on browser help you navigating through elements in a webpage
  - ```XPath``` is the path towards a specific part of a webpage
  - ```CSS selectors``` are first for styling web pages, but allows to match position of an element within HTML structures
- Typical scraping helpers: [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/) and [SelectorGadget](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?pli=1)

:::

::: {.notes}
**slide presenter**: Marine

First goes to the page and look at what a webpage looks like
:::

## Scraping a BIS speech with rvest

![](pictures/title.png){width=90% fig-align="center"}

:::{.notes}
**slide presenter**: Marine

Rather use directly scrapemate in the browser to show them
:::

## Scraping a BIS speech with rvest {.smaller .scrollable}

:::{.fragment}
```{r}
#| output-location: fragment
url_speech <- "https://www.bis.org/review/r241022f.htm"
page <- read_html(url_speech)
print(page)
```
:::

:::{.fragment}
```{r}
#| output-location: fragment
page %>% 
  html_element("h1") %>%
  html_text
```

:::

:::{.fragment}
```{r}
#| output-location: fragment
page %>% 
  html_element("#extratitle-div p:nth-child(1)") %>% 
  html_text
```

:::

:::{.fragment}
```{r}
#| output-location: fragment
page %>% 
  html_elements(".Reden") %>% 
  html_text
```
:::

:::{.notes}
**slide presenter**: Marine
:::

## Scraping BIS: understanding URLs

:::{.fragment .small-text}
- Our query on the list of speeches: [https://www.bis.org/cbspeeches/index.htm?fromDate=01%2F01%2F2022&cbspeeches_page=2&cbspeeches_page_length=25](https://www.bis.org/cbspeeches/index.htm?fromDate=01%2F01%2F2022&cbspeeches_page=2&cbspeeches_page_length=25)
:::

:::{.fragment}
```{r}
#| output-location: fragment
#| code-line-numbers: "1|2|3|4|6"
day <- "01"
month <- "10"
year <- 2024 # we want to look at all the speech since October 1st 2024
page <- 2
# The package glue allows to insert variables in a text string 
url_second_page <- glue("https://www.bis.org/cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page={page}&cbspeeches_page_length=25")
print(url_second_page)
```
:::

::: {.notes}
**slide presenter**: Aurélien

- Go to: "https://www.bis.org/cbspeeches/index.htm". Change date. Then display 25 speeches. And go to page 2.
- Often you may use both rvest and Rselenium
- Looking at the Website and the pattern of URL
:::

## Scraping the query page: mixing [rvest](https://rvest.tidyverse.org/) and [RSelenium](https://docs.ropensci.org/RSelenium/index.html)

```{r launching-bot}
#| eval: false
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
                   chromever = NULL,
                   port = 4444L) 
remote_driver <- driver[["client"]]
```

::: {.notes}
**slide presenter**: Aurélien
:::

## Scraping one page: mixing [rvest](https://rvest.tidyverse.org/) and [RSelenium](https://docs.ropensci.org/RSelenium/index.html)

::: {.fragment}
```{r navigating, eval=FALSE}
remote_driver$navigate(url_second_page)
Sys.sleep(session$delay)
```
:::

<br>

::: {.fragment}
```{r element-date, eval=FALSE}
#| output-location: fragment
element <- remote_driver$findElement("css selector", ".item_date")
element$getElementText()[[1]]
```

```{r}
#| echo: FALSE
date <- "24 Oct 2024"
print(date)
```

:::

<br>

::: {.fragment}
```{r all-elements, eval=FALSE}
#| output-location: fragment
elements <- remote_driver$findElements("css selector", ".item_date")
length(elements)
```

```{r}
#| echo: FALSE
length <- 25
print(length)
```
:::

::: {.fragment}
```{r extract-text, eval=FALSE}
#| output-location: fragment
elements[[25]]$getElementText()[[1]]
```

```{r}
#| echo: FALSE
last_date <- "15 Oct 2024"
print(last_date)
```

:::

::: {.notes}
**slide presenter**: Aurélien
:::

## Scraping one page {.smaller}

```{r scraping-page, eval=FALSE}
#| code-fold: true
data_page <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>% 
                      map_chr(., ~.$getElementText()[[1]]),
                    info = remote_driver$findElements("css selector", ".item_date+ td") %>% 
                      map_chr(., ~.$getElementText()[[1]]),
                    url = remote_driver$findElements("css selector", ".dark") %>% 
                      map_chr(., ~.$getElementAttribute("href")[[1]])) %>% 
  separate(info, c("title", "description", "speaker"), "\n")
```


```{r}
#| echo: FALSE
# Data from previous chunk has been saved: `saveRDS(data_page, here("slides", "data_page.rds"))`

data_page <- readRDS(here("slides", "data_page.rds"))

data_page %>% 
  datatable(rownames = FALSE,
     class = 'cell-border stripe',
     options = list(
       searching = TRUE, # This removes the search bar
       pageLength = 6, # This sets the initial number of rows displayed to 5
       dom = "tip")) %>%
  # Add custom style to reduce font size
  htmlwidgets::onRender("
    function(el, x) {
      $(el).css('font-size', '14px');  // Adjust the font size here
      $(el).find('th').css('font-size', '12px');  // Adjust the header font size
    }
  ")
```

::: {.notes}
**slide presenter**: Aurélien
:::


## Scraping all the pages {.smaller}

```{r all-pages, eval=FALSE}
starting_url <- glue("https://www.bis.org/cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page=1&cbspeeches_page_length=25")
remote_driver$navigate(starting_url)

# Extract the total number of pages
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText()[[1]] %>%
  str_remove_all("Page 1 of ") %>%
  as.integer()

# creating a list objet to allocate progressively information
metadata <- vector(mode = "list", length = nb_pages)

for(page in 1:nb_pages){
  url <- glue("https://www.bis.org/cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page={page}&cbspeeches_page_length=25")
  remote_driver$navigate(url)
  nod <- nod(session, url) # introducing politely to the new page
  Sys.sleep(session$delay) # using the delay time set by polite

  metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>% 
                            map_chr(., ~.$getElementText()[[1]]),
                          info = remote_driver$findElements("css selector", ".item_date+ td") %>% 
                            map_chr(., ~.$getElementText()[[1]]),
                          url = remote_driver$findElements("css selector", ".dark") %>% 
                            map_chr(., ~.$getElementAttribute("href")[[1]])) 
}

metadata <- bind_rows(metadata) %>% 
  separate(info, c("title", "description", "speaker"), "\n")
driver$server$stop() # we close the bot once we've finished
```

::: {.notes}
**slide presenter**: Aurélien
:::

# Exercises

## Exercises

:::{.incremental .highlight-last}
- Easy exercise: Scraping the UK Election Results ([https://www.bbc.com/news/election/2024/uk/results](https://www.bbc.com/news/election/2024/uk/results))
- Medium exercise: Scraping the state of EU legislation ([https://oeil.secure.europarl.europa.eu/oeil/popups/thematicnote.do?id=41380&l=en](https://oeil.secure.europarl.europa.eu/oeil/popups/thematicnote.do?id=41380&l=en))
- Difficult exercise: Scraping the ECB occasional papers (["https://www.ecb.europa.eu/press/research-publications/occasional-papers/html/index.en.html"](https://www.ecb.europa.eu/press/research-publications/occasional-papers/html/index.en.html))
:::

::: {.notes}
**slide presenter**: Aurélien
:::

## Easy exercise 

:::{.incremental .highlight-last}
- **You want to scrape and analyze results of the 2024 UK General Election from the BBC website.**

:::{.small-text}
- Objectives:
  - Count the number of parties with at least one seat.
  - Determine which parties won or lost compared with the previous election.
  - Calculate and visualize the average votes per seat for each parties with at least one seat.
  - Compare findings for the entire UK vs. England.

:::
:::

## Easy exercise {.smaller}  

### **Approach**

:::{.incremental .highlight-last .small-text}

 1. Use the rvest and polite packages to retrieve data from the [BBC website](https://www.bbc.com/news/election/2024/uk/results) for party names, seats, votes, and seat changes for all parties in the UK.
 
 2. Organize data into a dataframe and clean it. Converte seat and vote counts to numeric and remove extraneous symbols.

 3. Analysis:
  
  - Count number of parties with at least one seat.
  - Order the parties according to seat gains/losses.
  - Calculate votes per seat for each party with a least one seat.
  - Plot the number of votes per seat for all parties with a least one seat

 4. Repeat the  process for [parties in England](https://www.bbc.com/news/election/2024/uk/regions/E92000001) and then compare the results between the UK and England.

:::

## Medium exercise part 1 {.smaller .scrollable}

You want to know what happened to the files which were EU legislative priorities in 2023-2024

::: {.small-text}

- **1. Scrape the basic information**

We are going to list all relevant procedures. In the EU, once proposed each piece of legislation has a procedure number, including 'COD'. 

Go to the [page](https://oeil.secure.europarl.europa.eu/oeil/popups/thematicnote.do?id=41380&l=en) that lists the legislative files which were priorities for 2023-24

You have to scrape this page to obtain a data frame, in which there will be: 

- the title
- number 
- url towards the specific page of each procedure

Check one or two links 

- can you copy paste them in a browser and access the page?
- Is there anything missing in the URL? How could you fix this?

Search manually a procedure to find out how urls are made: https://oeil.secure.europarl.europa.eu/oeil/search/search.do?searchTab=y

**Tip**: you can use the `paste()` function. 
:::

## Medium exercise part 2

:::{.small-text}

- **2. Filter only the procedures of interest**

Now you have listed the names of all relevant procedures, and the links to access them. Create a data frame that contains only procedure with 'COD' in their reference number. 

**Tip**: you can use the the `str_detect()` function of `stringr`.
:::

## Medium exercise part 3 {.scrollable .smaller}

- **3. Scrape a single page**
Take this single URL link: https://oeil.secure.europarl.europa.eu/oeil/popups/ficheprocedure.do?reference=2021/0433(CNS)&l=en. 
It is one of the ones you have listed

In a separate data frame (which will have only one line, and three columns), scrape:

- the **status of the procedure** (i.e. at which stage it is)

- the **date at which the legislative file was published**

- the **date at which the EP took its decision**

**Tip**: for the dates first, select all the dates. 
Then, select the names of all the events to which they correspond. 
Finally, select your event of interest with `grepl()` (use for example "proposal")

## Medium exercise part 4 {.smaller .scrollable}

:::{.small-text .scrollable}
**4. Writing a function**
Write a function that automates the scraping you did at question 3. (generalize your code!). 
For each URL, the function has to scrape the same three pieces of information. 
Run that function and store the results in a data frame that also contains the number of the procedures and their URLs.

**Tips**: 

- Explanations about creating a function in R here: 
https://www.r-bloggers.com/2022/04/how-to-create-your-own-functions-in-r/

- In the function, you can use the `tibble()` function to bind the different information together.

- At the end, use `return()`. This indicates to R that it is the output of the function. 

- When applying the function, you may find the `lapply()` function useful!

- Don't hesitate to test the function on one or a handful of URLs! 

Some of the info you are looking for may not be on all pages. 
Use the function length() to check whether your code found something, and write "To check" if the information is not found. 
Why is some info missing on some pages?
:::

## Medium exercise part 5

**5. Explore the data**

:::{.small-text .scrollable}
- **Duration** Calculate the number of days between the legislative proposal and the EP decision in a new column of your data frame. 

Tip: you have to tell R that you are working with dates. 
Search which function allows to do this!

- **Missing data** What happens to the cases where the date of EP decision is not yet available? Pay attention when calculating the duration!

- **Spotting specific cases** Let's look for the longer process. When did that procedure start?
:::

## Difficult exercise {.smaller}

### Scraping the [ECB occasional papers]("https://www.ecb.europa.eu/press/research-publications/occasional-papers/html/index.en.html")

:::{.incremental .highlight-last}
- **Goal**: Create a database with the titles, authors, abstracts, JEL-codes, URLs, and date of publication of all the ECB occasional papers. Find the most recurrent words and expressions in the abstracts and titles.
- **Approach**: first look at the page and understand the structure of the page.
  - Is the page fully loaded when you access it? If not, you will need to scroll down to access all the papers.
  - Is the abstract or the JEL-codes visible? If not, you will need to click on a button.
  - Considering this, you may need to use ```RSelenium``` to scrape the page.
- Be careful to handle the particular structure of some information: for instance, you want to extract all the authors individually for each paper.
- **Tip**: check that you always have the right number of information
:::


# Resources

## Useful resources
- [Basic R learning](https://r4ds.had.co.nz/)
- [More advanced R learning](https://adv-r.hadley.nz/)
- [Starting with web scraping](https://practicewebscrapingsite.wordpress.com/)
- [Using GGraph](https://ggplot2-book.org/networks)
- [Complementary package for network analysis](https://agoutsmedt.github.io/networkflow/)
- [Good tutorial on scraping with RSelenium](https://www.rselenium-teaching.etiennebacher.com/#/title-slide)

## References
