---
title: "Mastering Web Scraping for Data Collection"
subtitle: "MethodsNET Workshop"
author: "Aurélien Goutsmedt, Thomas Laloux and Marine Bardou (UCLouvain)"
date: "10/31/2024"
format:
  revealjs: 
    theme: 
      [solarized, custom.scss]
    transition: convex
    slide-number: true
    toc: false 
    toc-depth: 1
    slide-level: 2
    number-sections: true
    number-depth: 1
    css: "custom.css"
    embed-resources: true
    view-distance: 100
    progress: true
    width: 1200
    margin: 0.01
    preview-links: true
    code-tools: true
    code-line-numbers: false
    code-copy: true
    code-link: true
    code-overflow: wrap
    code-block-height: 400px
lightbox: auto
bibliography: "bibliography.bib" 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, echo = TRUE)
```

# Training Goals

## Motivations {.smaller}

:::{.incremental .highlight-last}

### First Session:

- Giving a basic understanding of what web scraping is and what it can do
- Discussing ethical (and legal) issues linked to web scraping
- Proposing a roadmap for understanding how to proceed for practicing web scraping using R
- Providing bits of codes and practical tips

<br> 

### Second Session:

- Hands-on practice with different exercises by level of difficulty

:::

::: {.notes}

:::

## Requisits

:::{.incremental .highlight-last}
- Need of R and RStudio for the second session (please make sure to install them!)
- These slides are built from a ```.qmd``` (quarto) document $\Rightarrow$ all the codes used in these slides can be run in RStudio
:::

::: {.fragment}
```{r installing-packages}
# These lines of code has to be run before if you want to install all the packages directly

# pacman will be used to install (if necessary) and load packages
# We install pacman if it is not already installed
if(length(grep("pacman", installed.packages())) == 0) install.packages("pacman")
library(pacman)

# Installing the needed packages in advance
p_load(tidyverse, # basic suite of packages
       glue, # useful for building string (notably for url)
       scico, # color palettes
       patchwork, # for juxtaposition of graphs
       DT) # to display html tables
```
:::

```{r}
#| echo: FALSE
p_load(here)
```


# What is web Scraping

## What is web scraping ?

:::{.incremental .highlight-last}
:::{.small-text}
- The **web scraping** is a method for extracting data available in the World Wide Web 
- The World Wide Web, or "Web", is a network of websites (online documents coded in ```html``` and ```css```)
- A **web scraper** is a program, for instance in  ```R ```, that automatically read the ```html``` structure of a website and extract the relevant content (text, hypertext references, tables)
  - No need to fully understand ```html``` and ```css```
- Useful when many pages to scrape

:::
:::

## What is HTML and CSS?

:::{.columns}
:::{.column width="50%"}

![](exemple_wikipedia.png){width=91% fig-align="center"}

:::
:::{.column width="50%"}
:::{.fragment}

![](exemple_html.png){width=88% fig-align="center"}

:::
:::
:::

## API vs. web scraping

:::{.incremental .highlight-last .small-text}
:::{.fragment .nonincremental}
- **API (Application Programming Interface)** provides a structured and predictable way to retrieve data from a service. It's like ordering from a menu; you request specific data and receive it in a structured format
  - [Twitter API](https://developer.x.com/en/docs/x-api)
  - [Wikidata API](https://www.wikidata.org/wiki/Wikidata:REST_API)
  - API for bibliometric data like Scopus, Web of Science, Google Scholar...

:::
- Web Scraping is the process of programmatically extracting data from the web page's HTML itself. It's akin to manually copying information from a book; you decide what information you need and how to extract it

:::

## API vs. web scraping

:::{.incremental .highlight-last .small-text}
- **Control and Structure**: APIs offer structured access to data, whereas web scraping requires parsing HTML and often cleaning the data yourself.
- **Ease of Use**: Using an API can be simpler since it's designed for data access (but not always the case). Scraping requires dealing with HTML changes and is more prone to breaking.
- **Availability**: Not all websites offer an API, making web scraping a necessity in some cases.
- **Limitations and Authorization**: APIs often have rate limits and may require authentication, but approve access to the data. Web scraping can bypass these limits but might violate terms of service.
:::


## Forget about big data, small data is everywhere!

:::{.incremental .highlight-last}
- A large possibility of data you can collect:
  - official documents/speeches
  - agenda and meetings
  - list of personnel or experts in commission
  - laws or negotiations
- To take into account the development of pages over time, we can do it through the [Internet Archive](http://web.archive.org/)
:::

::: {.notes}
Examples of project:
- Creating a database of central banks documents
- Database of the meetings of commissioners
- Prosopographic database: scraping public profiles, notably via LinkedIn.

Other examples on our own research?

Do you have ideas of data you may be interested to collect? Why you did not do it?
:::

## Building databases {.smaller}

### Involves a series of questions:
:::{.incremental .highlight-last}
 
- What's your research question and which data would be appropriate to answer it?
- How much data to collect?
  - Trade-off between collecting a lot of information (which requires more time) and risking to miss some information at a later step
- How to scrape the data? In which format?
  - Interaction between extracting data properly in a first step or cleaning it in a second step
- What do you loose by doing it automatically rather than manually? (or the reverse)
- How to analyse/understand my new data?
- How to update my database?

:::

# The Ethics of Web Scraping

## Ethical considerations

:::{.incremental .highlight-last  .small-text}
- **Legal Considerations**: Not all data is free to scrape. Websites' terms of service may explicitly forbid web scraping, and in some jurisdictions, scraping can have legal implications
  - What is "forbidden" by a website is not necessary "illegal"
- **Privacy Concerns**: Scraping personal data can raise significant privacy issues and may be subject to regulations like GDPR in Europe
- **Website Performance**: Scraping, especially if aggressive (e.g., making too many requests in a short period), can negatively impact the performance of a website, affecting its usability for others
:::

## Questions at stake [@krotov2020]

![@krotov2020](legality_scraping.png){width=86% fig-align="center"}

:::{.notes}
- No legislation specific to web scraping exist.
  - May be impossible to determine the legality of some scraping actions.
- Uncertainty about violating the "terms of use" or "terms of service" is a "fraudulent" use of data. Often, should be motivated and not consider illegal as such. It has been also noted for instance that "Web Scraping publicly available data likely does not violate the CFAA". (CFAA = Computer Fraud and Abuse Act)
- Breach of contract: should have incurred material damages.
- Trespass to chattels: when have incurred material damages.
- Trade Secrets: when used for deliberate surveillance
:::


## Ethical practices

:::{.incremental .highlight-last}
- **Respect ```robots.txt```**: This file on websites indicates which parts should not be scraped
- **Rate Limiting**: Making requests at a reasonable rate to avoid overloading the website's server
- **User-Agent String**: Identifying your scraper can help website owners understand the nature of the traffic
- **Data Use**: Consider the ethical implications of how scraped data is used. Ensure it respects the privacy and rights of individuals

:::

# How to scrape a website? 

## What is R? 

:::{.incremental .highlight-last}
- programming language created at the end of 1990s for statistical computing 
- free and open source
- generally used in the development environment RStudio 

:::

## R basic concepts {.scrollable}

- **object**: like a box to which you assign a name and put various things in (from very simple to very complex)

Here is how you create an object in R:
```{r}
annas_age <- 26
annas_age
names_organizers <- c("Aurélien", "Thomas", "Marine")
names_organizers
```

## R basic concepts {.scrollable}

- **data frame** (it is a specific type of object!): comparable to a spreadsheet
```{r}
firstnames <- c("Anna", "Laura", "Lise")
firstnames
ages <- c(26,28,25)
ages
discipline <- c("maths", "sociology", "law")
discipline

my_data <- data.frame(firstnames, 
                      ages, 
                      discipline)
my_data
```

## R basic concepts 

- **function**: a block of code to which you provide input and which returns an output 
You can define yourself a function, but many are pre built. 
```{r}
length(names_organizers)
```

- **package**: most importantly, packages contain functions! 
```{r}
library(stringr)
str_detect(names_organizers, "Aurélien")
```

##  Useful packages for webscraping in R

:::{.incremental .highlight-last}
- [rvest](https://rvest.tidyverse.org/): navigating website, scraping and cleaning html code
- [polite](https://dmi3kno.github.io/polite/): responsible web etiquette (informing the website that you are scraping)
- [RSelenium](https://docs.ropensci.org/RSelenium/index.html): using a bot to interact with a website

:::{.fragment}
```{r}
p_load(rvest, # scraping and manipulating html pages
       polite, # scraping ethically
       RSelenium) # scraping by interacting with RSelenium
```

- Additional tool to navigate ```html``` and ```css``` code: [SelectorGadget](https://selectorgadget.com/)  or [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/)
:::
:::

## Navigating the website

:::{.incremental .highlight-last .small-text}

::: {.fragment}
```{r}
bis_website_path <- "https://www.bis.org/cbspeeches/index.htm"
```
:::

::: {.fragment .nonincremental}
- Two basic ways to scrape a series of pages:
  - Interacting with the website by using a bot browser (the more complicated one, using RSelenium)
  - Understanding/extracting the "structure" of the website (only need rvest in many cases)
:::

::: {.fragment .nonincremental}
- Cases in which you need to interact with the website
  - When you need to scroll to have the full content
  - When you need to open menus to display all the content
  - When you need to type in a search bar and run a search
  
:::
:::

::: {.notes}
Page BIS: we will extract date, titles, etc... But we don't want just the first page, so before to do that, we need to understand how to navigate between pages. 
:::

## The Role of Sitemaps

:::{.incremental .highlight-last .small-text}
- Sitemap: to inform search engines about URLs on a website that are available for web crawling
  - Understand the structure of a website
  - Find where is the information we want to extract

:::{.fragment .nonincremental}
- In general : 
  - `<domain>/sitemap.xml`
  - `<domain>/sitemap_index.xml`
  - `<domain>/sitemap`
  - for BIS : [https://www.bis.org/sitemap.xml](https://www.bis.org/sitemap.xml)

:::
:::

## Being respectful of the website {.smaller .scrollable}

::: {.fragment}

Declaring yourself:
```{r eval=TRUE}
session <- polite::bow(bis_website_path, 
                       user_agent = "polite R package - used for academic training by 
                       Aurélien Goutsmedt (aurelien.goutsmedt[at]uclouvain.be)")
```
:::

::: {.fragment}
```{r eval=TRUE}
#| output-location: fragment
cat(session$robotstxt$text)
```
:::

::: {.fragment}
```{r eval=TRUE}
#| output-location: fragment
session$robotstxt$sitemap
```
:::

:::{.notes}
Visit the sitemap
:::

## Using sitemap {.scrollable .smaller}

```{r eval=TRUE}
#| code-fold: true

# This function goes to a sitemap page, and extract all the urls found
extract_url_from_sitemap <- function(url, delay = 1) { 
  urls <- read_html(url) %>% 
    html_elements(xpath = ".//loc") %>% 
    html_text()
  Sys.sleep(delay) # You set a delay to avoid overloading the website
  return(urls)
}

# insistently allows to retry when you did not succeed in loading the page
insistently_extract_url <- insistently(extract_url_from_sitemap, 
                                       rate = rate_backoff(max_times = 5)) 

document_pages <- extract_url_from_sitemap(session$robotstxt$sitemap$value) %>% 
  .[str_detect(., "documents")] # We keep only the URLs for documents

bis_pages <- map(document_pages[1:5], # showing the code just on the first five years
                 ~insistently_extract_url(url = ., 
                                          delay = session$delay))

bis_pages <- tibble(year = str_extract(document_pages[1:5], "\\d{4}"),
                    urls = bis_pages) %>% 
  unnest(urls)
```


```{r eval=TRUE}
#| echo: FALSE
datatable(bis_pages,
          rownames = FALSE,
          class = 'cell-border stripe',
          options = list(
            searching = FALSE, # This removes the search bar
            pageLength = 10, # This sets the initial number of rows displayed to 5
            dom = "tip")) %>%
  # Add custom style to reduce font size
  htmlwidgets::onRender("
    function(el, x) {
      $(el).css('font-size', '14px');  // Adjust the font size here
      $(el).find('th').css('font-size', '12px');  // Adjust the header font size
    }
  ")
```

## The key steps of web scraping 

There are many ways to scrape, but your baseline scenario is:

:::{.incremental .highlight-last}
1. **Read the html code** of the page 
(you can access this page with an url directly, or after interacting with it using RSelenium). 

2. **Extract** some specific elements in that page using selectors. 

3. **Read the text** (or some other info) behind contained in these elements. 

4. **Store** the information retrieved in a data frame. 
:::

## Scraping a BIS speech with rvest {.smaller .scrollable}

![Example of Speech](text_BIS.png){width=86% fig-align="center"}

## Scraping a BIS speech with rvest {.smaller .scrollable}

![Example of Speech](title.png){width=86% fig-align="center"}

## Scraping a BIS speech with rvest {.smaller .scrollable}

:::{.fragment}
```{r}
#| output-location: fragment
url_speech <- "https://www.bis.org/review/r241022f.htm"
page <- read_html(url_speech)
print(page)
```
:::

:::{.fragment}
```{r}
#| output-location: fragment
page %>% 
  html_element("h1") %>%
  html_text
```

:::

:::{.fragment}
```{r}
#| output-location: fragment
page %>% 
  html_element("#extratitle-div p:nth-child(1)") %>% 
  html_text
```

:::

:::{.fragment}
```{r}
#| output-location: fragment
page %>% 
  html_elements(".Reden") %>% 
  html_text
```
:::

## Scraping BIS: understanding URLs

:::{.fragment .small-text}
- Our query on the list of speeches:

[https://www.bis.org/cbspeeches/index.htm?fromDate=01%2F01%2F2022&cbspeeches_page=2&cbspeeches_page_length=25](https://www.bis.org/cbspeeches/index.htm?fromDate=01%2F01%2F2022&cbspeeches_page=2&cbspeeches_page_length=25)
:::

:::{.fragment}
```{r}
#| output-location: fragment
#| code-line-numbers: "|5"
page <- 2
day <- "01"
month <- "10"
year <- 2024 # we want to look at all the speech since October 1st 2024
url_second_page <- glue("https://www.bis.org/cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page={page}&cbspeeches_page_length=25")

print(url_second_page)
```
:::

::: {.notes}
- Go to: "https://www.bis.org/cbspeeches/index.htm". Change date. Then display 25 speeches. And go to page 2.
- Often you may use both rvest and Rselenium
- Looking at the Website and the pattern of URL
:::

## Scraping one page: using a scrape helper

:::{.incremental .highlight-last}
- Scraping add-ons on browser help you navigating through elements in a webpage
  - ```XPath``` is the path towards a specific part of a webpage
  - ```CSS selectors``` are first for styling web pages, but allows to match position of an element within HTML structures
- Typical scraping helpers: [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/) and [SelectorGadget](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?pli=1)

:::

::: {.notes}
First goes to the page and look at what a webpage looks like
:::

## Scraping the query page: mixing [rvest](https://rvest.tidyverse.org/) and [RSelenium](https://docs.ropensci.org/RSelenium/index.html)

```{r launching-bot}
#| eval: false
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
                   chromever = NULL,
                   port = 4444L) 
remote_driver <- driver[["client"]]
```

## Scraping one page: mixing [rvest](https://rvest.tidyverse.org/) and [RSelenium](https://docs.ropensci.org/RSelenium/index.html)

::: {.fragment}
```{r navigating, eval=FALSE}
remote_driver$navigate(url_second_page)
Sys.sleep(session$delay)
```
:::

<br>

::: {.fragment}
```{r element-date, eval=FALSE}
#| output-location: fragment
element <- remote_driver$findElement("css selector", ".item_date")
element$getElementText()[[1]]
```

```{r}
#| echo: FALSE
date <- "09 Oct 2024"
print(date)
```

:::

<br>

::: {.fragment}
```{r all-elements, eval=FALSE}
#| output-location: fragment
elements <- remote_driver$findElements("css selector", ".item_date")
length(elements)
```

```{r}
#| echo: FALSE
length <- 25
print(length)
```
:::

::: {.fragment}
```{r extract-text, eval=FALSE}
#| output-location: fragment
elements[[25]]$getElementText()[[1]]
```

```{r}
#| echo: FALSE
last_date <- "02 Oct 2024"
print(last_date)
```

:::

## Scraping one page {.smaller}

```{r scraping-page, eval=FALSE}
#| code-fold: true
data_page <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>% 
                      map_chr(., ~.$getElementText()[[1]]),
                    info = remote_driver$findElements("css selector", ".item_date+ td") %>% 
                      map_chr(., ~.$getElementText()[[1]]),
                    url = remote_driver$findElements("css selector", ".dark") %>% 
                      map_chr(., ~.$getElementAttribute("href")[[1]])) %>% 
  separate(info, c("title", "description", "speaker"), "\n")
```


```{r}
#| echo: FALSE
# Data from previous chunk has been saved: `saveRDS(data_page, here("slides", "data_page.rds"))`

data_page <- readRDS(here("slides", "data_page.rds"))

data_page %>% 
  datatable(rownames = FALSE,
     class = 'cell-border stripe',
     options = list(
       searching = TRUE, # This removes the search bar
       pageLength = 6, # This sets the initial number of rows displayed to 5
       dom = "tip")) %>%
  # Add custom style to reduce font size
  htmlwidgets::onRender("
    function(el, x) {
      $(el).css('font-size', '14px');  // Adjust the font size here
      $(el).find('th').css('font-size', '12px');  // Adjust the header font size
    }
  ")
```


## Scraping all the pages {.smaller}

```{r all-pages, eval=FALSE}
starting_url <- glue("https://www.bis.org/cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page=1&cbspeeches_page_length=25")
remote_driver$navigate(starting_url)

# Extract the total number of pages
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText()[[1]] %>%
  str_remove_all("Page 1 of ") %>%
  as.integer()

# creating a list objet to allocate progressively information
metadata <- vector(mode = "list", length = nb_pages)

for(page in 1:nb_pages){
  url <- glue("https://www.bis.org/cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page={page}&cbspeeches_page_length=25")
  remote_driver$navigate(url)
  nod <- nod(session, url) # introducing politely to the new page
  Sys.sleep(session$delay) # using the delay time set by polite

  metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>% 
                            map_chr(., ~.$getElementText()[[1]]),
                          info = remote_driver$findElements("css selector", ".item_date+ td") %>% 
                            map_chr(., ~.$getElementText()[[1]]),
                          url = remote_driver$findElements("css selector", ".dark") %>% 
                            map_chr(., ~.$getElementAttribute("href")[[1]])) 
}

metadata <- bind_rows(metadata) %>% 
  separate(info, c("title", "description", "speaker"), "\n")
driver$server$stop() # we close the bot once we've finished
```



# Exercises

## Exercises

:::{.incremental .highlight-last}
- Easy exercise: Scraping the UK Election Results ([https://www.bbc.com/news/election/2024/uk/results](https://www.bbc.com/news/election/2024/uk/results))
- Medium exercise: Scraping the state of EU legislation ([https://oeil.secure.europarl.europa.eu/oeil/popups/thematicnote.do?id=41380&l=en](https://oeil.secure.europarl.europa.eu/oeil/popups/thematicnote.do?id=41380&l=en))
- Difficult exercise: Scraping the ECB occasional papers (["https://www.ecb.europa.eu/press/research-publications/occasional-papers/html/index.en.html"](https://www.ecb.europa.eu/press/research-publications/occasional-papers/html/index.en.html))
:::

## Easy exercise 

- **You want to scrape and analyze results of the 2024 UK General Election from the BBC website.**

:::{.incremental .highlight-last .small-text}
- Objectives:
  - Count the number of parties with at least one seat.
  - Determine which parties won or lost compared with the previous election.
  - Calculate and visualize the average votes per seat for each parties with at least one seat.
  - Compare findings for the entire UK vs. England.
  
:::
:::

## Easy exercise {.smaller}  

- **Approach**


:::{.incremental .highlight-last .small-text}


 1. Use the rvest and polite packages to retrieve data from the BBC website for party names, seats, votes, and seat changes for all parties in the UK.
 
 2. Organize data into a dataframe and clean it. Converte seat and vote counts to numeric and remove extraneous symbols.

 3. Analysis:
  
  - Count number of parties with at least one seat.
  - Order the parties according to seat gains/losses.
  - Calculate votes per seat for each party with a least one seat.
  - Plot the number of votes per seat for all parties with a least one seat

 4. Repeat the  process for England and then compare the results between the UK and England.

:::



## Medium exercise part 1 {.smaller .scrollable}
You want to know what happened to the files which were EU legislative priorities in 2023-2024

::: {.small-text .scrollable}
**1. Scrape the basic information**

We are going to list all relevant procedures. In the EU, once proposed each piece of legislation has a procedure number, including 'COD'. 

Go to this page that lists the legislative files which were priorities for 2023-24: https://oeil.secure.europarl.europa.eu/oeil/popups/thematicnote.do?id=41380&l=en 

You have to scrape this page to obtain a data frame, in which there will be: 

- the title

- number 

- url towards the specific page of each procedure

Check one or two links - can you copy paste them in a browser and access the page?
Is there anything missing in the URL? How could you fix this?
Search manually a procedure to find out how urls are made: https://oeil.secure.europarl.europa.eu/oeil/search/search.do?searchTab=y

**Tip**: you can use the `paste()` function. 
:::

## Medium exercise part 2

:::{.small-text}
**2. Filter only the procedures of interest**
Now you have listed the names of all relevant procedures, and the links to access them. Create a data frame that contains only procedure with 'COD' in their reference number. 

**Tip**: you can use the the `str_detect()` function of `stringr`.
:::

## Medium exercise part 3 {.scrollable .smaller}

**3. Scrape a single page**
Take this single URL link: https://oeil.secure.europarl.europa.eu/oeil/popups/ficheprocedure.do?reference=2021/0433(CNS)&l=en. 
It is one of the ones you have listed

In a separate data frame (which will have only one line, and three columns), scrape:

- the **status of the procedure** (i.e. at which stage it is)

- the **date at which the legislative file was published**

- the **date at which the EP took its decision**

**Tip**: for the dates first, select all the dates. 
Then, select the names of all the events to which they correspond. 
Finally, select your event of interest with `grepl()` (use for example "proposal")

## Medium exercise part 4 {.smaller .scrollable}

:::{.small-text .scrollable}
**4. Writing a function**
Write a function that automates the scraping you did at question 3. (generalize your code!). 
For each URL, the function has to scrape the same three pieces of information. 
Run that function and store the results in a data frame that also contains the number of the procedures and their URLs.

**Tips**: 

- Explanations about creating a function in R here: 
https://www.r-bloggers.com/2022/04/how-to-create-your-own-functions-in-r/

- In the function, you can use the `tibble()` function to bind the different information together.

- At the end, use `return()`. This indicates to R that it is the output of the function. 

- When applying the function, you may find the `lapply()` function useful!

- Don't hesitate to test the function on one or a handful of URLs! 

Some of the info you are looking for may not be on all pages. 
Use the function length() to check whether your code found something, and write "To check" if the information is not found. 
Why is some info missing on some pages?
:::

## Medium exercise part 5

**5. Explore the data**

:::{.small-text .scrollable}
- **Duration** Calculate the number of days between the legislative proposal and the EP decision in a new column of your data frame. 

Tip: you have to tell R that you are working with dates. 
Search which function allows to do this!

- **Missing data** What happens to the cases where the date of EP decision is not yet available? Pay attention when calculating the duration!

- **Spotting specific cases** Let's look for the longer process. When did that procedure start?
:::

# Resources

## Useful resources
- [Basic R learning](https://r4ds.had.co.nz/)
- [More advanced R learning](https://adv-r.hadley.nz/)
- [Starting with web scraping](https://practicewebscrapingsite.wordpress.com/)
- [Using GGraph](https://ggplot2-book.org/networks)
- [Complementary package for network analysis](https://agoutsmedt.github.io/networkflow/)
- [Good tutorial on scraping with RSelenium](https://www.rselenium-teaching.etiennebacher.com/#/title-slide)

## References